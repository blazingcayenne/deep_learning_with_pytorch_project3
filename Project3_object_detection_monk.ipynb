{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Project 3: Object Detection</font>\n",
    "\n",
    "\n",
    "#### Maximum Points: 100\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Plot Ground Truth Bounding Boxes</h3></td> <td><h3>20</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Training</h3></td> <td><h3>25</h3></td> </tr>\n",
    "        <tr><td><h3>3</h3></td> <td><h3>Inference</h3></td> <td><h3>15</h3></td> </tr>\n",
    "        <tr><td><h3>4</h3></td> <td><h3>COCO Detection Evaluation</h3></td> <td><h3>25</h3></td> </tr>\n",
    "        <tr><td><h3>5</h3></td> <td><h3>Run Inference on a Video</h3></td> <td><h3>15</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:orange\">Project Approach</font>\n",
    "\n",
    "In the last project, I wrote a framework, which allowed me to quickly create classification experiments. My motivation was not only to fulfill the project's requirements, but also to learn how to program in the Python language. In this project, I want to use an existing framework to minimize the amount of code I write. I decided to use [Monk](https://github.com/Tessellate-Imaging/Monk_Object_Detection)'s [RetinaNet] implementation (https://www.paperswithcode.com/method/retinanet), which is built upon [PyTorch RetinaNet](https://github.com/yhenon/pytorch-retinanet).\n",
    "\n",
    "![RetinaNet network architecture](https://www.paperswithcode.com/media/methods/Screen_Shot_2020-06-07_at_4.22.37_PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:orange\">Monk Installation</font>\n",
    "\n",
    "To install Monk, run the following commands.\n",
    "\n",
    "```\n",
    "git clone https://github.com/Tessellate-Imaging/Monk_Object_Detection.git\n",
    "cd Monk_Object_Detection/5_pytorch_retinanet/installation && cat requirements_brule.txt | xargs -n 1 -L 1 pip install\n",
    "```\n",
    "\n",
    "The output should be similar to the following.\n",
    "\n",
    "```\n",
    "Cloning into 'Monk_Object_Detection'...\n",
    "remote: Enumerating objects: 10525, done.\n",
    "remote: Total 10525 (delta 0), reused 0 (delta 0), pack-reused 10525\n",
    "Receiving objects: 100% (10525/10525), 260.87 MiB | 539.00 KiB/s, done.\n",
    "Resolving deltas: 100% (4692/4692), done.\n",
    "Checking out files: 100% (8428/8428), done.\n",
    "Collecting cython\n",
    "  Downloading Cython-0.29.21-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
    "     |████████████████████████████████| 2.0 MB 17.8 MB/s eta 0:00:01\n",
    "Installing collected packages: cython\n",
    "Successfully installed cython-0.29.21\n",
    "Collecting efficientnet_pytorch\n",
    "  Downloading efficientnet_pytorch-0.7.0.tar.gz (20 kB)\n",
    "...\n",
    "```\n",
    "\n",
    "**Note:** `requirements.txt` is outdated and this caused a conflict between PyTorch and TorchVision. I created `requirements_brule.txt` to install modules are not installed on my _Deep Learning with PyTorch_ Docker image. I suspect `requirements_colab.txt` is also outdated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://github.com/Tessellate-Imaging/Monk_Object_Detection.git\n",
    "#! cd Monk_Object_Detection/5_pytorch_retinanet/installation && cat requirements_brule.txt | xargs -n 1 -L 1 pip install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:purple\">Download the Dataset</font> \n",
    "\n",
    "**[Download the Vehicle registration plate](https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1)**\n",
    "\n",
    "\n",
    "Download the Vehicle Registration Plate dataset from [here](https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1) and unzip it. \n",
    "\n",
    "We will have the following directory structure:\n",
    "\n",
    "```\n",
    "Dataset\n",
    "├── train\n",
    "│   └── Vehicle registration plate\n",
    "│       └── Label\n",
    "└── validation\n",
    "    └── Vehicle registration plate\n",
    "        └── Label\n",
    "```\n",
    "\n",
    "Unzipping the file will give you a directory `Dataset`. This directory has two folder `train` and `validation`. Each train and validation folder has `Vehicle registration plate`  folder with `.jpg` images and a folder `Labels`.  `Labels` folder has bounding box data for the images.\n",
    "\n",
    "\n",
    "For example,\n",
    "For image: `Dataset/train/Vehicle registration plate/bf4689922cdfd532.jpg`\n",
    "Label file is  `Dataset/train/Vehicle registration plate/Label/bf4689922cdfd532.txt`\n",
    "\n",
    "There are one or more lines in each `.txt` file. Each line represents one bounding box.\n",
    "For example,\n",
    "```\n",
    "Vehicle registration plate 385.28 445.15 618.24 514.225\n",
    "Vehicle registration plate 839.68 266.066462 874.24 289.091462\n",
    "```\n",
    "\n",
    "We have a single class detection (`Vehicle registration plate detection`) problem. So bounding box details start from the fourth column in each row.\n",
    "\n",
    "Representation is in `xmin`, `ymin`, `xmax`, and `ymax` format.\n",
    "\n",
    "**It has `5308` training and `386` validation dataset.**\n",
    "\n",
    "Data is downloaded from [Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Download/Unzip Commands</font>\n",
    "\n",
    "To install the dataset, run the following commands.\n",
    "\n",
    "```\n",
    "wget -O Dataset.zip https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1\n",
    "unzip Dataset.zip\n",
    "rm Dataset.zip\n",
    "```\n",
    "\n",
    "The output should be similar to the following.\n",
    "\n",
    "\n",
    "```\n",
    "--2021-02-10 19:58:37--  https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1\n",
    "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
    "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
    "HTTP request sent, awaiting response... 301 Moved Permanently\n",
    "...\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 1764437533 (1.6G) [application/binary]\n",
    "Saving to: ‘Dataset.zip’\n",
    "\n",
    "Dataset.zip         100%[===================>]   1.64G  25.2MB/s    in 45s     \n",
    "\n",
    "2021-02-10 19:59:24 (37.2 MB/s) - ‘Dataset.zip’ saved [1764437533/1764437533]\n",
    "\n",
    "Archive:  Dataset.zip\n",
    "   creating: Dataset/\n",
    "   creating: Dataset/validation/\n",
    "   creating: Dataset/validation/Vehicle registration plate/\n",
    "  inflating: Dataset/validation/Vehicle registration plate/52ceb1fc30b413e5.jpg  \n",
    "  inflating: Dataset/validation/Vehicle registration plate/182268e1f8c6525f.jpg\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget -O Dataset.zip https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1\n",
    "#! unzip Dataset.zip\n",
    "#! rm Dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "from IPython.utils import io       \n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"Monk_Object_Detection/5_pytorch_retinanet/lib/\")\n",
    "from infer_detector import Infer\n",
    "from train_detector import Detector\n",
    "from retinanet.dataloader import CocoDataset, Resizer, Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Convert Dataset to COCO Format</font>\n",
    "\n",
    "[Monk](https://github.com/Tessellate-Imaging/Monk_Object_Detection) supports datasets in two formats: CSV and COCO. The `DatasetHelper` class converts the _Vehicle Registration Plate_ dataset to COCO Format 2.\n",
    "\n",
    "**Note:** This conversion is destructive. It replaces the original dataset with the following directory structure.\n",
    "\n",
    "```\n",
    "                                            Monk Parameters\n",
    "                                            ------------------------\n",
    "Dataset                                  →  root_dir\n",
    "└── license_plates                       →  coco_dir\n",
    "    ├── images_train                     →  set_dir\n",
    "    │   ├── 00009e5b390986a0.jpg\n",
    "    │   ├── 000228608388803f.jpg\n",
    "    │   └── ........ (and so on)\n",
    "    ├── images_valid                     →  set_dir\n",
    "    │   ├── 003a5aaf6d17c917.jpg\n",
    "    │   ├── 00723dac8201a83e.jpg\n",
    "    │   └── ........ (and so on)\n",
    "    └── annotations\n",
    "        ├── instances_images_train.json  →  instances_<set_dir>.json\n",
    "        ├── instances_images_valid.json  →  instances_<set_dir>.json\n",
    "        └── classes.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_dir = \"./runs\"       # the root directory for TensorBoard logs\n",
    "data_dir = \"./Dataset\"    # the root directory for the CoCo dataset\n",
    "\n",
    "\n",
    "class DatasetHelper(object):\n",
    "    def __init__(self, data_dir):\n",
    "        self.__data_dir = data_dir\n",
    "        self.__orig_train_dir = os.path.join(data_dir, \"train\")\n",
    "        self.__orig_train_image_dir = os.path.join(data_dir, \"train\", \"Vehicle registration plate\")\n",
    "        self.__orig_train_label_dir = os.path.join(data_dir, \"train\", \"Vehicle registration plate\", \"Label\")\n",
    "        self.__orig_valid_dir = os.path.join(data_dir, \"validation\")\n",
    "        self.__orig_valid_image_dir = os.path.join(data_dir, \"validation\", \"Vehicle registration plate\")\n",
    "        self.__orig_valid_label_dir = os.path.join(data_dir, \"validation\", \"Vehicle registration plate\", \"Label\")\n",
    "        self.__coco_annotations_dir = os.path.join(data_dir, \"license_plates\", \"annotations\")\n",
    "        self.__coco_train_image_dir = os.path.join(data_dir, \"license_plates\", \"images_train\")\n",
    "        self.__coco_valid_image_dir = os.path.join(data_dir, \"license_plates\", \"images_valid\")\n",
    "        self.__coco_class_list_path = os.path.join(self.__coco_annotations_dir, \"classes.txt\")\n",
    "        self.__coco_train_annotation_path = os.path.join(self.__coco_annotations_dir, \"instances_images_train.json\")\n",
    "        self.__coco_valid_annotation_path = os.path.join(self.__coco_annotations_dir, \"instances_images_valid.json\")\n",
    "\n",
    "    def __is_data_set_valid(self):\n",
    "        paths_and_counts = [\n",
    "            (self.__data_dir, 2),\n",
    "            (self.__orig_train_dir, 1),\n",
    "            (self.__orig_train_image_dir, 5309),\n",
    "            (self.__orig_train_label_dir, 5308),\n",
    "            (self.__orig_valid_dir, 1),\n",
    "            (self.__orig_valid_image_dir, 387),\n",
    "            (self.__orig_valid_label_dir, 386)\n",
    "        ]\n",
    "       \n",
    "        for path, count in paths_and_counts:\n",
    "            if not os.path.isdir(path) or len(os.listdir(path)) != count:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def __process_image(self, img_name, img_id, img_path):\n",
    "        img = Image.open(img_path)\n",
    "        json = {\n",
    "            \"file_name\": img_name,\n",
    "            \"height\": img.size[1],\n",
    "            \"id\": img_id,\n",
    "            \"width\": img.size[0]\n",
    "        }\n",
    "        del img\n",
    "        return json\n",
    "\n",
    "    def __process_annotations(self, img_id, ann_id, ann_path):\n",
    "        anns = []\n",
    "        f = open(ann_path, 'r')\n",
    "        for line in f.readlines():\n",
    "            tokens = line.split(' ')\n",
    "            xmin = float(tokens[3])\n",
    "            ymin = float(tokens[4])\n",
    "            xmax = float(tokens[5])\n",
    "            ymax = float(tokens[6])\n",
    "            assert xmax > xmin\n",
    "            assert ymax > ymin\n",
    "            width = abs(xmax - xmin)\n",
    "            height = abs(ymax - ymin)\n",
    "            anns.append({\n",
    "                \"area\": width * height,\n",
    "                \"bbox\": [xmin, ymin, width, height],\n",
    "                \"category_id\": 0,\n",
    "                \"id\": ann_id,\n",
    "                \"ignore\": 0,\n",
    "                \"image_id\": img_id,\n",
    "                \"iscrowd\": 0,\n",
    "                \"segmentation\": []\n",
    "            })\n",
    "            ann_id += 1\n",
    "        f.close()\n",
    "        return anns\n",
    "        \n",
    "    def __process_data(self, orig_image_dir, orig_label_dir, coco_image_dir, coco_annotation_path):\n",
    "        json_dict = {\n",
    "            \"type\": \"instances\", \n",
    "            \"images\": [], \n",
    "            \"categories\": [{\"supercategory\": \"none\", \"name\": \"license plate\", \"id\": 0}],\n",
    "            \"annotations\": []\n",
    "        }\n",
    "        \n",
    "        img_id = 0\n",
    "        ann_id = 0\n",
    "        for img_name in os.listdir(orig_image_dir):\n",
    "            orig_image_path = os.path.join(orig_image_dir, img_name)\n",
    "            orig_label_path = os.path.join(orig_label_dir, os.path.splitext(img_name)[0] + \".txt\")\n",
    "            coco_image_path = os.path.join(coco_image_dir, img_name)\n",
    "            \n",
    "            if os.path.isfile(orig_image_path):\n",
    "                # create image and annotation JSON objects\n",
    "                img = self.__process_image(img_name, img_id, orig_image_path)\n",
    "                anns = self.__process_annotations(img_id, ann_id, orig_label_path)\n",
    "                json_dict[\"images\"].append(img)\n",
    "                json_dict[\"annotations\"].extend(anns)\n",
    "                img_id += 1\n",
    "                ann_id += len(anns)\n",
    "\n",
    "                # move image file\n",
    "                if os.path.isfile(orig_image_path):\n",
    "                   os.rename(orig_image_path, coco_image_path)\n",
    "        \n",
    "        # create json annotation file\n",
    "        f = open(coco_annotation_path, 'w')\n",
    "        f.write(json.dumps(json_dict))\n",
    "        f.close()\n",
    "        \n",
    "    def __create_class_list(self):\n",
    "        f = open(self.__coco_class_list_path, 'w')\n",
    "        f.write(\"license plate\")\n",
    "        f.close()\n",
    "    \n",
    "    def convert_to_coco(self):\n",
    "        if not self.__is_data_set_valid():\n",
    "            raise IOError(f\"'{data_dir}' is not a valid Vehicle Registration Plate dataset.\")\n",
    "\n",
    "        # create COCO directories\n",
    "        os.makedirs(self.__coco_annotations_dir)\n",
    "        os.makedirs(self.__coco_train_image_dir)\n",
    "        os.makedirs(self.__coco_valid_image_dir)\n",
    "\n",
    "        # create class list & training/validation annotation files\n",
    "        self.__create_class_list()\n",
    "        self.__process_data(\n",
    "            self.__orig_train_image_dir, \n",
    "            self.__orig_train_label_dir, \n",
    "            self.__coco_train_image_dir,\n",
    "            self.__coco_train_annotation_path\n",
    "        )\n",
    "        self.__process_data(\n",
    "            self.__orig_valid_image_dir, \n",
    "            self.__orig_valid_label_dir, \n",
    "            self.__coco_valid_image_dir,\n",
    "            self.__coco_valid_annotation_path\n",
    "        )\n",
    "        \n",
    "        # remove original directories\n",
    "        shutil.rmtree(self.__orig_train_dir)\n",
    "        shutil.rmtree(self.__orig_valid_dir)\n",
    "\n",
    "    @property\n",
    "    def valid_image_dir(self):\n",
    "        return self.__coco_valid_image_dir\n",
    "        \n",
    "    def __get_samples(self, annotation_path, ids):\n",
    "        f = open(annotation_path, 'r')\n",
    "        data = json.load(f)\n",
    "        f.close()\n",
    "        imgs = [img for img in data[\"images\"] if img[\"id\"] in ids]\n",
    "        anns = [ann for ann in data[\"annotations\"] if ann[\"image_id\"] in ids]\n",
    "        return [(img, [ann for ann in anns if ann[\"image_id\"] == img[\"id\"]]) for img in imgs]\n",
    "            \n",
    "    def get_train_samples(self, ids):\n",
    "        return self.__get_samples(self.__coco_train_annotation_path, ids)\n",
    "\n",
    "    def get_valid_samples(self, ids):\n",
    "        return self.__get_samples(self.__coco_valid_annotation_path, ids)\n",
    "\n",
    "dataset_helper = DatasetHelper(data_dir)\n",
    "try:\n",
    "    dataset_helper.convert_to_coco()\n",
    "    print(\"Converted dataset.\")\n",
    "except:\n",
    "    print(\"Dataset already converted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Dataset Analysis</font>\n",
    "\n",
    "It is useful to understand the dataset's image dimensions when creating transforms. The following cell visualizes the image widths, heights, and aspect ratios as histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_dims():\n",
    "    # create list of training and validation samples\n",
    "    samples = []\n",
    "    samples.extend(dataset_helper.get_train_samples(list(range(5308))))\n",
    "    samples.extend(dataset_helper.get_valid_samples(list(range(386))))\n",
    "\n",
    "    def get_image_dims(sample):\n",
    "        img = sample[0]\n",
    "        width = img[\"width\"]\n",
    "        height = img[\"height\"]\n",
    "        return (width, height, float(width)/height)    \n",
    "\n",
    "    image_dims = tuple(zip(*[get_image_dims(sample) for sample in samples]))\n",
    "    \n",
    "    def plot_histogram(ax, xlabel, data):\n",
    "        min = np.min(data)\n",
    "        max = np.max(data)\n",
    "        ave = np.mean(data)\n",
    "        title = f\"{xlabel} Histogram (Min: {min:.2f}, Max: {max:.2f}, Ave: {ave:.2f})\"\n",
    "        ax.hist(data, facecolor=(1., 0.549, 0))\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.set_title(title)\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1)\n",
    "    fig.set_figwidth(6)\n",
    "    fig.set_figheight(9)\n",
    "\n",
    "    xlabels = (\"Width\", \"Height\", \"Aspect Ratio\")\n",
    "    for ax, xlabel, data in zip(axs, xlabels, image_dims):\n",
    "        plot_histogram(ax, xlabel, data)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    del samples\n",
    "    del image_dims\n",
    "\n",
    "\n",
    "analyze_image_dims()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Dataset and Transform Experimentation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class AlbumentationsCocoDataset(CocoDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.load_image(idx)\n",
    "        annot = self.load_annotations(idx)\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=img, bboxes=annot)\n",
    "            img = transformed[\"image\"]\n",
    "            annot = transformed[\"bboxes\"]\n",
    "        return {'img': img, 'annot': annot}\n",
    "    \n",
    "    def load_image(self, image_index):\n",
    "        image_info = self.coco.loadImgs(self.image_ids[image_index])[0]\n",
    "        path = os.path.join(self.root_dir, self.img_dir, self.set_name, image_info['file_name'])\n",
    "        img = skimage.io.imread(path)\n",
    "\n",
    "        if len(img.shape) == 2:\n",
    "            img = skimage.color.gray2rgb(img)\n",
    "\n",
    "        return img #.astype(np.float32)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CoCoDataset\")\n",
    "dataset1 = CocoDataset(\n",
    "    root_dir = os.path.join(data_dir, \"license_plates\"),\n",
    "    img_dir = \"./\",\n",
    "    set_dir = \"images_valid\",\n",
    "    transform = transforms.Compose([Normalizer(), Resizer()])\n",
    ")\n",
    "\n",
    "sample = dataset1[5]\n",
    "img, annot = sample['img'], sample['annot']\n",
    "print()\n",
    "print(sample.keys())\n",
    "print(annot, type(annot))\n",
    "print(img.shape, type(img))\n",
    "\n",
    "# split into boxes and labels\n",
    "#boxes = annot[:, :4]\n",
    "#labels = (annot[:, 4] + 0.5).type(torch.int64)\n",
    "#print()\n",
    "#print(boxes)\n",
    "#print(labels)\n",
    "\n",
    "# join back into a single tensor\n",
    "#join = torch.cat((boxes, labels[:, None]), dim=1)\n",
    "#print()\n",
    "#print(f\"annot: {join}\")\n",
    "\n",
    "print()\n",
    "print(\"AlbumentationsCocoDataset\")\n",
    "dataset2 = AlbumentationsCocoDataset(\n",
    "    root_dir = os.path.join(data_dir, \"license_plates\"),\n",
    "    img_dir = \"./\",\n",
    "    set_dir = \"images_valid\",\n",
    "    transform = A.Compose([\n",
    "        A.SmallestMaxSize(max_size=608),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc'))\n",
    ")\n",
    "\n",
    "sample = dataset2[5]\n",
    "img, annot = sample['img'], sample['annot']\n",
    "rows, cols, cns = img.shape\n",
    "\n",
    "print()\n",
    "print(sample.keys())\n",
    "print(annot, type(annot))\n",
    "print(img.shape, type(img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font style=\"color:green\">1. Plot Ground Truth Bounding Boxes [20 Points]</font> \n",
    "\n",
    "**You have to show three images from validation data with the bounding boxes.**\n",
    "\n",
    "The plotted images should be similar to the following:\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g1.png'>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g2.png'>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g3.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Annotator</font>\n",
    "\n",
    "[Monk](https://github.com/Tessellate-Imaging/Monk_Object_Detection) is capable of annotating objects. However, its functionality is limited, the annotations are not aesthetically pleasing, and it is incapable of meeting this project's requirements, e.g., annotating in-memory memory frames. Consequently, I wrote my own `Annotator` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotator(object):\n",
    "    def __init__(self, image_dir):\n",
    "        self.__image_dir = image_dir\n",
    "        \n",
    "    def __annotate_image(self, img_bgr, bboxes, scores=None, color=(255, 140, 0)):\n",
    "        scale = 1\n",
    "        thick = 1\n",
    "        white = (255, 255, 255)\n",
    "        for idx, bbox in enumerate(bboxes):\n",
    "            xminb, yminb, xmaxb, ymaxb = bbox\n",
    "            xminb = int(xminb + 0.5)\n",
    "            yminb = int(yminb + 0.5)\n",
    "            xmaxb = int(xmaxb + 0.5)\n",
    "            ymaxb = int(ymaxb + 0.5)\n",
    "            cv2.rectangle(img_bgr, (xminb, yminb), (xmaxb, ymaxb), color, 5)\n",
    "\n",
    "            text = \"lic. plate\"\n",
    "            if scores is not None:\n",
    "                text = format(scores[idx], \".0%\")\n",
    "            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_DUPLEX, scale, thick)\n",
    "\n",
    "            pad = 8\n",
    "            xmint = xminb\n",
    "            ymint = yminb - text_size[1] - pad\n",
    "            xmaxt = xminb + text_size[0] + pad\n",
    "            ymaxt = yminb\n",
    "            xtext = xmint + int(pad / 2)\n",
    "            ytext = ymint + text_size[1] + int(pad / 2)\n",
    "            cv2.rectangle(img_bgr, (xmint, ymint), (xmaxt, ymaxt), color, 5)\n",
    "            cv2.rectangle(img_bgr, (xmint, ymint), (xmaxt, ymaxt), color, -1)\n",
    "            cv2.putText(img_bgr, text, (xtext, ytext), cv2.FONT_HERSHEY_DUPLEX, scale, white, thick)\n",
    "        return img_bgr\n",
    "    \n",
    "    def __to_min_max_bbox(self, gt_bbox):\n",
    "        xmin, ymin, width, height = gt_bbox\n",
    "        return [xmin, ymin, xmin + width, ymin + height]\n",
    "\n",
    "    def annotate_sample(self, sample, color=(255, 140, 0)):\n",
    "        img, anns = sample\n",
    "        img_bgr = cv2.imread(os.path.join(self.__image_dir, img[\"file_name\"]))\n",
    "        bboxes = [self.__to_min_max_bbox(ann[\"bbox\"]) for ann in anns]\n",
    "        return self.__annotate_image(img_bgr, bboxes, None, color)\n",
    "    \n",
    "    def annotate_image_bgr_pred(self, img_bgr, bboxes, scores, color=(255, 140, 0), vis_threshold = 0.4):\n",
    "        mask = scores > vis_threshold\n",
    "        return self.__annotate_image(img_bgr, bboxes[mask], scores[mask], color)\n",
    "    \n",
    "    def annotate_image_name_pred(self, img_name, bboxes, scores, color=(255, 140, 0), vis_threshold = 0.4):\n",
    "        img_bgr = cv2.imread(os.path.join(self.__image_dir, img_name))\n",
    "        return self.annotate_image_bgr_pred(img_bgr, bboxes, scores, color, vis_threshold)\n",
    "\n",
    "    @classmethod\n",
    "    def show_image(cls, img):\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "annotator = Annotator(dataset_helper.valid_image_dir)    \n",
    "samples = dataset_helper.get_valid_samples([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(annotator.annotate_sample(samples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(annotator.annotate_sample(samples[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(annotator.annotate_sample(samples[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Model Evaluator</font>\n",
    "\n",
    "[Monk](https://github.com/Tessellate-Imaging/Monk_Object_Detection) supplies an `evaluate_coco(dataset, model, threshold=0.05)` function, which evaluates the model against the validation data and prints the COCO metrics. Unfortunately, this function does not return the COCO metrics, so the mAP metric cannot be logged to TensorBoard. Consequently, I modified the aforementioned function to create the `ModelEvaluator` class, which provides the following features.\n",
    "\n",
    "* Suppresses the (print) output from the `pycocotools.cocoeval.COCOeval` support class.\n",
    "* Returns a tuple of the COCO metrics and its primary metric, mean average precision (mAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator(object):\n",
    "    def __init__(self, model, dataset):\n",
    "        self._model = model\n",
    "        self._dataset = dataset\n",
    "        \n",
    "    def Evaluate(self, results_path=None, threshold=0.05):\n",
    "        if results_path is None:\n",
    "            results_path = f\"{self._dataset.set_name}_results.json\"\n",
    "\n",
    "        self._model.eval()\n",
    "        with torch.no_grad():\n",
    "            results = []\n",
    "            image_ids = []\n",
    "            data_len = len(self._dataset)\n",
    "            bar_format = \"{l_bar}{bar}| {n_fmt:>4}/{total_fmt:4} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\"\n",
    "            tqdm_pbar = tqdm(desc=\"CoCoEval\", bar_format=bar_format, total=data_len+1, mininterval=1., unit=\"image\")\n",
    "            for data_idx, data in enumerate(self._dataset):\n",
    "                tqdm_pbar.update(1)\n",
    "                \n",
    "                # run network\n",
    "                scores, labels, bboxes = self._model(data['img'].permute(2, 0, 1).cuda().float().unsqueeze(dim=0))\n",
    "                scores = scores.cpu()\n",
    "                labels = labels.cpu()\n",
    "                bboxes = bboxes.cpu()\n",
    "                \n",
    "                if bboxes.shape[0] > 0:\n",
    "                    # correct boxes for image scale and change to COCO format\n",
    "                    bboxes /= data[\"scale\"]\n",
    "                    bboxes[:, 2] -= bboxes[:, 0]\n",
    "                    bboxes[:, 3] -= bboxes[:, 1]\n",
    "\n",
    "                    for bbox_idx in range(bboxes.shape[0]):\n",
    "                        score = float(scores[bbox_idx])\n",
    "                        label = int(labels[bbox_idx])\n",
    "                        bbox  = bboxes[bbox_idx, :]\n",
    "\n",
    "                        # since scores are sorted, we can jump out of this loop\n",
    "                        # once we reach a score that is smaller than the threshold\n",
    "                        if score < threshold:\n",
    "                            break\n",
    "\n",
    "                        results.append({\n",
    "                            \"image_id\"    : self._dataset.image_ids[data_idx],\n",
    "                            \"category_id\" : self._dataset.label_to_coco_label(label),\n",
    "                            \"score\"       : float(score),\n",
    "                            \"bbox\"        : bbox.tolist(),\n",
    "                        })\n",
    "\n",
    "                # append image to list of processed images\n",
    "                image_ids.append(self._dataset.image_ids[data_idx])\n",
    "\n",
    "            mean_ap = 0.\n",
    "            coco_eval = None\n",
    "            if len(results) > 0:\n",
    "                with io.capture_output() as captured: # surpress output\n",
    "                    # write output\n",
    "                    json.dump(results, open(results_path, 'w'), indent=4)\n",
    "\n",
    "                    # load results in COCO evaluation tool\n",
    "                    coco_true = self._dataset.coco\n",
    "                    coco_pred = coco_true.loadRes(results_path) # why write, then read?\n",
    "\n",
    "                    # run COCO evaluation\n",
    "                    coco_eval = COCOeval(coco_true, coco_pred, \"bbox\")\n",
    "                    coco_eval.params.imgIds = image_ids\n",
    "                    coco_eval.evaluate()\n",
    "                    coco_eval.accumulate()\n",
    "                    coco_eval.summarize()\n",
    "                    mean_ap = coco_eval.stats[0].item()\n",
    "            \n",
    "            tqdm_pbar.set_postfix(mAP=f\"{mean_ap:.03f}\")\n",
    "            tqdm_pbar.update(1)\n",
    "            tqdm_pbar.close()\n",
    "            return coco_eval, mean_ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font style=\"color:green\">2. Training [25 Points]</font> \n",
    "\n",
    "- **Write your training code in this section.**\n",
    "\n",
    "- **You also have to share ([shared logs example](https://tensorboard.dev/experiment/JRtnsKbwTaq1ow6nPLPGeg)) the loss plot of your training using tensorboard.dev.** \n",
    "\n",
    "How to share TensorBoard logs using tensorboard.dev find [here](https://courses.opencv.org/courses/course-v1:OpenCV+OpenCV-106+2019_T1/courseware/b1c43ffe765246658e537109e188addb/d62572ec8bd344db9aeae81235ede618/4?activate_block_id=block-v1%3AOpenCV%2BOpenCV-106%2B2019_T1%2Btype%40vertical%2Bblock%40398b46ddcd5c465fa52cb4d572ba3229)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">\"Refactoring\" Monk Detector Class (via Inheritance)</font>\n",
    "\n",
    "The Monk `Detector` class' `Train` method prints training progress to the cell's output window. This project requires the use of [TensorBoard](https://www.tensorflow.org/tensorboard/). Rather than refactor the Monk framework, I derived a new detector class, `MyDetector`, from Monk's `Detector` class and that adds the following features.\n",
    "\n",
    "* Progress is displayed using `tqdm` progress bars.\n",
    "* Optimizer and LR scheduler may be specified (using `Adam` and `ReduceLROnPlateau`).\n",
    "* Average loss is computed on the validation data each epoch.\n",
    "* The model is saved each time the validation loss is minimized.\n",
    "* Training terminates after N consecutive epochs where the validation loss increases.\n",
    "* The following parameters and metrics are logged to TensorBoard.\n",
    "  * learning rate\n",
    "  * training class loss\n",
    "  * training regression loss\n",
    "  * training total loss\n",
    "  * validation class loss\n",
    "  * validation regression loss\n",
    "  * validation total loss\n",
    "  * validation mAP\n",
    "\n",
    "Monk's data augmentation for RetinaNet is limited. It randomly flips the training data horizontally. If I am not able to achieve the mAP goal of 0.50 with a simpler RetinaNet model that does not overfit the training data, then I will have to implement and/or integrate additional augmentation transforms.\n",
    "\n",
    "Prior to the training loop, the validation loss is computed on the pretrained model. The following operations are performed each epoch.\n",
    "\n",
    "* Training cycle that computes losses on the training data and performs back propogation.\n",
    "* Validation cycle that computes losses on the validation data.\n",
    "* Evaluation cycle that computes mAP on the validation data.\n",
    "* Scheduler update.\n",
    "* Training termination check.\n",
    "\n",
    "Since Monk does not support data subsets, the training pipeline was validated by temporary code that prematurely terminating the training and validation cycles. The `ReduceLROnPlateau` scheduler and the training termination code was tested by temporary code that simulated overfitting by monotonically increasing the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stats(object):\n",
    "    def __init__(self):\n",
    "        self._n = 0\n",
    "        self._total = 0.\n",
    "\n",
    "    @property\n",
    "    def n(self):\n",
    "        return self._n\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        if self._n == 0:\n",
    "            return 0.\n",
    "        return self._total / self._n\n",
    "\n",
    "    def add(self, x):\n",
    "        self._n += 1\n",
    "        self._total += x\n",
    "\n",
    "\n",
    "class MyDetector(Detector):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.__model_evaluator = None\n",
    "\n",
    "    @property\n",
    "    def ModelParameters(self):\n",
    "        return self.system_dict[\"local\"][\"model\"].parameters()\n",
    "        \n",
    "    @property\n",
    "    def ModelEvaluator(self):\n",
    "        if self.__model_evaluator is None:\n",
    "            self.__model_evaluator = ModelEvaluator(\n",
    "                self.system_dict[\"local\"][\"model\"],\n",
    "                self.system_dict[\"local\"][\"dataset_val\"]\n",
    "            )\n",
    "        return self.__model_evaluator\n",
    "        \n",
    "    def Set_Hyperparams(self, lr=0.0001, val_interval=1, print_interval=20):\n",
    "        \"\"\"\n",
    "        Use SetOptimizerAndScheduler to initialize the optimizer and LR scheduler.\n",
    "        \"\"\"\n",
    "        return\n",
    "        \n",
    "    def SetOptimizerAndScheduler(self, optimizer, scheduler):\n",
    "        self.system_dict[\"local\"][\"optimizer\"] = optimizer\n",
    "        self.system_dict[\"local\"][\"scheduler\"] = scheduler\n",
    "        \n",
    "    def Load(self, model_path=\"final_model.pt\"):\n",
    "        self.system_dict[\"local\"][\"model\"] = torch.load(model_path)\n",
    "        if torch.cuda.is_available():\n",
    "            self.system_dict[\"local\"][\"model\"] = self.system_dict[\"local\"][\"model\"].cuda();\n",
    "        \n",
    "    def Train(self, num_epochs, exper_name, no_best_thd=10):\n",
    "        saved_model = exper_name + \".pt\"\n",
    "        log_dir = os.path.join(runs_dir, exper_name)\n",
    "        self.system_dict[\"output\"][\"saved_model\"] = saved_model;\n",
    "        self.system_dict[\"params\"][\"num_epochs\"] = num_epochs;\n",
    "        \n",
    "        model = self.system_dict[\"local\"][\"model\"]\n",
    "        device = self.system_dict[\"local\"][\"device\"]\n",
    "        optimizer = self.system_dict[\"local\"][\"optimizer\"]\n",
    "        scheduler = self.system_dict[\"local\"][\"scheduler\"]\n",
    "        dataloader_train = self.system_dict[\"local\"][\"dataloader_train\"]\n",
    "        dataloader_valid = self.system_dict[\"local\"][\"dataloader_val\"]\n",
    "\n",
    "        writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "        def TrainCycle(epoch_num):\n",
    "            train_closs, train_rloss, train_tloss = MyDetector._TrainCycle(\n",
    "                f\"Train {epoch_num:02d}\", \n",
    "                model, \n",
    "                device, \n",
    "                dataloader_train, \n",
    "                optimizer\n",
    "            )\n",
    "            writer.add_scalar(\"training/class_loss\", train_closs, epoch_num)\n",
    "            writer.add_scalar(\"training/regression_loss\", train_rloss, epoch_num)\n",
    "            writer.add_scalar(\"training/total_loss\", train_tloss, epoch_num)\n",
    "            return train_tloss\n",
    "\n",
    "        def ValidCycle(epoch_num):\n",
    "            valid_closs, valid_rloss, valid_tloss = MyDetector._ValidCycle(\n",
    "                f\"Valid {epoch_num:02d}\", \n",
    "                model, \n",
    "                device, \n",
    "                dataloader_valid\n",
    "            )\n",
    "            \n",
    "            writer.add_scalar(\"validation/class_loss\", valid_closs, epoch_num)\n",
    "            writer.add_scalar(\"validation/regression_loss\", valid_rloss, epoch_num)\n",
    "            writer.add_scalar(\"validation/total_loss\", valid_tloss, epoch_num)\n",
    "            return valid_tloss\n",
    "            \n",
    "        epochs_since_best = 0\n",
    "        min_valid_loss = ValidCycle(0)\n",
    "        for epoch_num in range(1, num_epochs + 1):\n",
    "            train_loss = TrainCycle(epoch_num)\n",
    "            valid_loss = ValidCycle(epoch_num)\n",
    "\n",
    "            coco_eval, mean_ap = self.ModelEvaluator.Evaluate()\n",
    "            writer.add_scalar(\"test/mAP\", mean_ap, epoch_num)\n",
    "\n",
    "            writer.add_scalar(\"learning_rate\", optimizer.param_groups[0]['lr'], epoch_num)\n",
    "            \n",
    "            if isinstance(scheduler, sched.ReduceLROnPlateau):\n",
    "                scheduler.step(valid_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "            if min_valid_loss > valid_loss:\n",
    "                epochs_since_best = 0\n",
    "                min_valid_loss = valid_loss\n",
    "                torch.save(model, saved_model)\n",
    "            else:\n",
    "                epochs_since_best += 1\n",
    "                if no_best_thd is not None and epochs_since_best >= no_best_thd:\n",
    "                    break\n",
    "        \n",
    "        writer.close()\n",
    "        model.eval()\n",
    "\n",
    "    @classmethod\n",
    "    def _CreateProgBarIter(cls, desc, dataloader):\n",
    "        bar_format = \"{l_bar}{bar}| {n_fmt:>4}/{total_fmt:4} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\"\n",
    "        return tqdm(dataloader, bar_format=bar_format, desc=desc, mininterval=1., unit=\"batch\")\n",
    "\n",
    "    @classmethod\n",
    "    def _TrainCycle(cls, tqdm_desc, model, device, dataloader, optimizer):\n",
    "        model.train()\n",
    "        \n",
    "        cstats = Stats(); rstats = Stats(); tstats = Stats()\n",
    "        tqdm_dataloader = MyDetector._CreateProgBarIter(tqdm_desc, dataloader)\n",
    "        for iter_num, data in enumerate(tqdm_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            img = data[\"img\"].to(device).float()\n",
    "            ann = data[\"annot\"].to(device)\n",
    "            closs, rloss = model([img, ann])\n",
    "\n",
    "            closs = closs.mean()\n",
    "            rloss = rloss.mean()\n",
    "            tloss = closs + rloss\n",
    "\n",
    "            tloss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            cstats.add(float(closs))\n",
    "            rstats.add(float(rloss))\n",
    "            tstats.add(float(tloss))\n",
    "            loss_text = f\"{{C:{cstats.mean:.3f}, R:{rstats.mean:.3f}, T:{tstats.mean:.3f}}}\"\n",
    "            tqdm_dataloader.set_postfix(losses=loss_text)\n",
    "\n",
    "            del closs\n",
    "            del rloss\n",
    "\n",
    "        tqdm_dataloader.close()\n",
    "        return cstats.mean, rstats.mean, tstats.mean\n",
    "\n",
    "    @classmethod\n",
    "    def _ValidCycle(cls, tqdm_desc, model, device, dataloader):\n",
    "        # Note: If model is set to evaluation mode, then it returns scores, labels and bboxes.\n",
    "        #       We want classification and regression loss, so we set it to training mode and\n",
    "        #       use torch.no_grad() to avoid gradient computation.\n",
    "        model.train()\n",
    "        cstats = Stats(); rstats = Stats(); tstats = Stats()\n",
    "        tqdm_dataloader = MyDetector._CreateProgBarIter(tqdm_desc, dataloader)\n",
    "        for iter_num, data in enumerate(tqdm_dataloader):\n",
    "            img = data[\"img\"].to(device).float()\n",
    "            ann = data[\"annot\"].to(device)\n",
    "            with torch.no_grad():        \n",
    "                closs, rloss = model([img, ann])\n",
    "\n",
    "            closs = closs.mean()\n",
    "            rloss = rloss.mean()\n",
    "            tloss = closs + rloss\n",
    "\n",
    "            cstats.add(float(closs))\n",
    "            rstats.add(float(rloss))\n",
    "            tstats.add(float(tloss))\n",
    "            loss_text = f\"{{C:{cstats.mean:.3f}, R:{rstats.mean:.3f}, T:{tstats.mean:.3f}}}\"\n",
    "            tqdm_dataloader.set_postfix(losses=loss_text)\n",
    "\n",
    "            del closs\n",
    "            del rloss\n",
    "\n",
    "        tqdm_dataloader.close()\n",
    "        return cstats.mean, rstats.mean, tstats.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Training Experiments</font>\n",
    "\n",
    "Training experiments are conducted via the `conduct_experiment(model_name, exper_name)` function. The `model_name` parameter may be one of the following values. The `exper_name` parameter is the experiment name. The name of the saved model is the experiment name with a .pt extension.\n",
    "\n",
    "* resnet18\n",
    "* resnet34\n",
    "* resnet50\n",
    "* resnet101\n",
    "* resnet152\n",
    "\n",
    "TQDM progress bars are used to display training progress. The following output is the first three epochs of training the resnet18 model.\n",
    "\n",
    "```\n",
    "Valid 00: 100%|██████████|   39/39   [00:23<00:00,  1.66batch/s, losses={C:1.034, R:0.979, T:2.012}]\n",
    "Train 01: 100%|██████████|  531/531  [12:12<00:00,  1.38s/batch, losses={C:0.315, R:0.508, T:0.823}]\n",
    "Valid 01: 100%|██████████|   39/39   [00:23<00:00,  1.69batch/s, losses={C:0.290, R:0.426, T:0.716}]\n",
    "CoCoEval: 100%|██████████|  387/387  [01:04<00:00,  5.99image/s, mAP=0.352]\n",
    "Train 02: 100%|██████████|  531/531  [12:12<00:00,  1.38s/batch, losses={C:0.174, R:0.333, T:0.507}]\n",
    "Valid 02: 100%|██████████|   39/39   [00:23<00:00,  1.67batch/s, losses={C:0.181, R:0.328, T:0.509}]\n",
    "CoCoEval: 100%|██████████|  387/387  [01:04<00:00,  5.97image/s, mAP=0.431]\n",
    "Train 03: 100%|██████████|  531/531  [12:12<00:00,  1.38s/batch, losses={C:0.132, R:0.282, T:0.415}]\n",
    "Valid 03: 100%|██████████|   39/39   [00:22<00:00,  1.72batch/s, losses={C:0.203, R:0.298, T:0.501}]\n",
    "CoCoEval: 100%|██████████|  387/387  [01:04<00:00,  5.98image/s, mAP=0.447]\n",
    "```\n",
    "\n",
    "The TensorBoard logs may be viewed by clicking this [link]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiment(model_name, exper_name):\n",
    "    trainer = MyDetector()\n",
    "    batch_size = {\n",
    "        \"resnet18\": 10,\n",
    "        \"resnet34\": 8,\n",
    "        \"resnet50\": 6,\n",
    "        \"resnet101\": 4,\n",
    "        \"resnet152\": 2\n",
    "    }[model_name]\n",
    "    \n",
    "    with io.capture_output() as captured: # surpress output\n",
    "        trainer.Train_Dataset(\n",
    "            root_dir = data_dir,          # path to root directory containing coco_dir\n",
    "            coco_dir = \"license_plates\",  # name of coco_dir contianing image and annotation folders\n",
    "            img_dir = \"./\",               # name of folder containing all training and validation folders\n",
    "            set_dir = \"images_train\",     # name of folder contain all training images\n",
    "            batch_size = batch_size,      # mini batch sampling size for training epochs\n",
    "            image_size = 512,             # must be 300 or 512\n",
    "            use_gpu = True,               # train on GPU if true; otherwise, train on CPU\n",
    "            num_workers = 12              # number of parallel processors for data loader\n",
    "        )\n",
    "\n",
    "        trainer.Val_Dataset(\n",
    "            root_dir = data_dir,          # path to root directory containing coco_dir\n",
    "            coco_dir = \"license_plates\",  # name of coco_dir contianing image and annotation folders\n",
    "            img_dir = \"./\",               # name of folder containing all training and validation folders\n",
    "            set_dir = \"images_valid\"\n",
    "        )\n",
    "\n",
    "        trainer.Model(model_name=model_name)   \n",
    "\n",
    "    optimizer = optim.Adam(trainer.ModelParameters, lr=0.0001)\n",
    "    scheduler = sched.ReduceLROnPlateau(optimizer, factor=math.sqrt(0.1), patience=3, threshold=0.001)\n",
    "    trainer.SetOptimizerAndScheduler(optimizer, scheduler)\n",
    "    trainer.Train(num_epochs=99, exper_name=exper_name)\n",
    "    \n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# uncomment the following lines to run an experiment\n",
    "conduct_experiment(\"resnet18\", \"RetinaNet18\")\n",
    "conduct_experiment(\"resnet34\", \"RetinaNet34\")\n",
    "conduct_experiment(\"resnet50\", \"RetinaNet50\")\n",
    "conduct_experiment(\"resnet101\", \"RetinaNet101\")\n",
    "conduct_experiment(\"resnet152\", \"RetinaNet152\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font style=\"color:green\">3. Inference [15 Points]</font> \n",
    "\n",
    "**You have to make predictions from your trained model on three images from the validation dataset.**\n",
    "\n",
    "The plotted images should be similar to the following:\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p1.png'>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p2.png'>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p3.png'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">\"Refactoring\" Monk Inference Class (via Inheritance)</font>\n",
    "\n",
    "The Monk `Infer` class' Predict method only accepts a path to an image. It also does not annotate images as nicely as my `Annotator`.  Rather than refactor the Monk framework, I derived a new inference class, `MyInferencer`, from Monk's `Infer` class that adds the following features.\n",
    "\n",
    "* Predictions may be performed on RBG and BGR in-memory images.\n",
    "* Predictions and result annotation may be performed on validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyInferencer(Infer):\n",
    "    def __init__(self, image_dir, annotator, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.system_dict[\"dataset\"] = {};\n",
    "        self.system_dict[\"dataset\"][\"val\"] = {};\n",
    "        self.__image_dir = image_dir\n",
    "        self.__annotator = annotator\n",
    "        self.__model_evaluator = None\n",
    "        \n",
    "    @property\n",
    "    def ModelEvaluator(self):\n",
    "        if self.__model_evaluator is None:\n",
    "            self.__model_evaluator = ModelEvaluator(\n",
    "                self.system_dict[\"local\"][\"model\"],\n",
    "                self.system_dict[\"local\"][\"dataset_val\"]\n",
    "            )\n",
    "        return self.__model_evaluator\n",
    "\n",
    "    def Val_Dataset(self, root_dir, coco_dir, img_dir, set_dir):\n",
    "        self.system_dict[\"dataset\"][\"val\"][\"status\"] = True;\n",
    "        self.system_dict[\"dataset\"][\"val\"][\"root_dir\"] = root_dir;\n",
    "        self.system_dict[\"dataset\"][\"val\"][\"coco_dir\"] = coco_dir;\n",
    "        self.system_dict[\"dataset\"][\"val\"][\"img_dir\"] = img_dir;\n",
    "        self.system_dict[\"dataset\"][\"val\"][\"set_dir\"] = set_dir;  \n",
    "\n",
    "        self.system_dict[\"local\"][\"dataset_val\"] = CocoDataset(\n",
    "            self.system_dict[\"dataset\"][\"val\"][\"root_dir\"] + \"/\" + self.system_dict[\"dataset\"][\"val\"][\"coco_dir\"], \n",
    "            img_dir = self.system_dict[\"dataset\"][\"val\"][\"img_dir\"], \n",
    "            set_dir = self.system_dict[\"dataset\"][\"val\"][\"set_dir\"],\n",
    "            transform = transforms.Compose([Normalizer(), Resizer()])\n",
    "        )\n",
    "\n",
    "    def PredictViaImageRGB(self, img):\n",
    "        # normalize image\n",
    "        image = img.astype(np.float32) / 255.;\n",
    "        image = (image.astype(np.float32) - self.system_dict[\"local\"][\"mean\"]) / self.system_dict[\"local\"][\"std\"];\n",
    "       \n",
    "        # compute scaling factor\n",
    "        rows, cols, cns = image.shape\n",
    "        smallest_side = min(rows, cols)\n",
    "        scale = self.system_dict[\"local\"][\"min_side\"] / smallest_side\n",
    "        largest_side = max(rows, cols)\n",
    "        if largest_side * scale > self.system_dict[\"local\"][\"max_side\"]:\n",
    "            scale = self.system_dict[\"local\"][\"max_side\"]  / largest_side\n",
    "\n",
    "        # resize and pad the image\n",
    "        image = skimage.transform.resize(image, (int(round(rows*scale)), int(round((cols*scale)))))\n",
    "        rows, cols, cns = image.shape\n",
    "        pad_w = 32 - rows%32\n",
    "        pad_h = 32 - cols%32\n",
    "        new_image = np.zeros((rows + pad_w, cols + pad_h, cns)).astype(np.float32)\n",
    "        new_image[:rows, :cols, :] = image.astype(np.float32)\n",
    "\n",
    "        # convert image to tensor and perform prediction\n",
    "        img = torch.from_numpy(new_image)\n",
    "        with torch.no_grad():\n",
    "            model = self.system_dict[\"local\"][\"model\"]\n",
    "            scores, labels, boxes = model(img.cuda().permute(2, 0, 1).float().unsqueeze(dim=0))\n",
    "            boxes /= scale\n",
    "\n",
    "        return scores, labels, boxes\n",
    "        \n",
    "    def PredictViaImageBGR(self, img):\n",
    "        return self.PredictViaImageRGB(img[:, :, ::-1])\n",
    "        \n",
    "    def PredictViaImageName(self, img_name):\n",
    "        img_path = os.path.join(self.__image_dir, img_name)\n",
    "        return self.PredictViaImageRGB(skimage.io.imread(img_path))\n",
    "    \n",
    "    def PredictAndAnnotateSample(self, sample, annotator, vis_threshold=0.4):\n",
    "        img, _ = sample\n",
    "        img_name = img[\"file_name\"]\n",
    "        scores, _, bboxes = self.PredictViaImageName(img_name)\n",
    "        return self.__annotator.annotate_image_name_pred(\n",
    "            img_name = img_name, \n",
    "            bboxes = bboxes, \n",
    "            scores = scores, \n",
    "            vis_threshold = vis_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Detecting and Annotating License Plates in Sample Validation Images</font>\n",
    "\n",
    "The following cell creates an \"inferencer\", loads the trained model, and annotates predictions on the same three samples used in Section 1 to plot ground truth bounding boxes. The subsequent three cells perform a prediction on each sample and annotate the detected license plates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exper_name = \"RetinaNet18\"\n",
    "inferencer = MyInferencer(dataset_helper.valid_image_dir, Annotator(dataset_helper.valid_image_dir))\n",
    "inferencer.Model(model_path=f\"{exper_name}.pt\")\n",
    "samples = dataset_helper.get_valid_samples([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(inferencer.PredictAndAnnotateSample(samples[0], annotator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(inferencer.PredictAndAnnotateSample(samples[1], annotator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(inferencer.PredictAndAnnotateSample(samples[2], annotator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font style=\"color:green\">4. COCO Detection Evaluation [25 Points]</font> \n",
    "\n",
    "**You have to evaluate your detection model on COCO detection evaluation metric.**\n",
    "\n",
    "For your reference here is the coco evaluation metric chart:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/03/c3-w9-coco_metric.png\">\n",
    "\n",
    "---\n",
    "\n",
    "#### <font style=\"color:red\">The expected `AP` (primary challenge metric) is more than `0.5`.</font>\n",
    "\n",
    "**The expected output should look similar to the following:**\n",
    "\n",
    "```\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.550\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.886\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.629\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.256\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.653\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.627\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.504\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.629\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.633\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.380\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.722\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.704\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Detection Model Evaluation</font>\n",
    "\n",
    "The following cell initializes the inferencer's validation dataset and requests its `ModelEvaluator`. It then calls the model evaluator's `Evaluate` method and summarizes the resulting CoCo evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.capture_output() as captured: # surpress output\n",
    "    inferencer.Val_Dataset(\n",
    "        root_dir = data_dir,          # path to root directory containing coco_dir\n",
    "        coco_dir = \"license_plates\",  # name of coco_dir contianing image and annotation folders\n",
    "        img_dir = \"./\",               # name of folder containing all training and validation folders\n",
    "        set_dir = \"images_valid\"\n",
    "    )\n",
    "\n",
    "evaluator = inferencer.ModelEvaluator\n",
    "coco_eval, _ = evaluator.Evaluate()   \n",
    "coco_eval.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:green\">5. Run Inference on a Video [15 Points]</font>\n",
    "\n",
    "#### [Download the Input Video](https://www.dropbox.com/s/g88o4dx18zpgn8k/projet3-input-video.mp4?dl=1)\n",
    "\n",
    "**You have to run inference on a video.** \n",
    "\n",
    "You can download the video from [here](https://www.dropbox.com/s/g88o4dx18zpgn8k/projet3-input-video.mp4?dl=1).\n",
    "\n",
    "#### <font style=\"color:red\">Upload the output video on youtube and share the link. Do not upload the video in the lab.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo, display\n",
    "video = YouTubeVideo(\"18HWHCevFdU\", width=640, height=360)\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your output video should have a bounding box around the vehicle registration plate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = YouTubeVideo(\"5SgCuee7AMs\", width=640, height=360)\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Detection and Annotating a Video</font>\n",
    "\n",
    "I modified the sample function to read the source video frame-by-frame, detect and annotate license plates in each frame, and create an output video. I uploaded this video to YouTube and it may be viewed in this notebook's last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_video(src_video_path, dst_video_path, frames_dir=None):\n",
    "    \"\"\"\n",
    "    Create a new video by annotating license plates in the source video.\n",
    "    \n",
    "    src_video_path (str): path of source video in which to annotate\n",
    "    dst_video_path (str): path to destination video in which to create\n",
    "    frames_dir optional(str): if specified, direction to write annotated frames\n",
    "    \"\"\"\n",
    "    \n",
    "    # create a video reader\n",
    "    reader = cv2.VideoCapture(src_video_path)\n",
    "    if not reader.isOpened(): \n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "        \n",
    "    # get source video attributes\n",
    "    width = int(reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frames_per_second = float(reader.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # create a video writer\n",
    "    writer = cv2.VideoWriter(\n",
    "        filename = dst_video_path,\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "        fps =  frames_per_second,\n",
    "        frameSize = (width, height),\n",
    "        isColor=True,\n",
    "    )\n",
    "    \n",
    "    # for each frame in the source video ...\n",
    "    count = 0\n",
    "    while reader.isOpened():\n",
    "        # read the frame\n",
    "        ret, frame = reader.read()\n",
    "        if ret:\n",
    "            # detect and annotate license plates\n",
    "            scores, _, bboxes = inferencer.PredictViaImageBGR(frame)\n",
    "            frame = annotator.annotate_image_bgr_pred(frame, bboxes, scores, color=(0, 140, 255))\n",
    "            # write frame to destination video\n",
    "            writer.write(frame)\n",
    "            # if specified, write frame to frames directory\n",
    "            if frames_dir is not None:\n",
    "                name = f\"frame{count:04d}.jpg\"\n",
    "                path = os.path.join(frames_dir, name)\n",
    "                cv2.imwrite(path, frame)\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # free reader and writer resources\n",
    "    reader.release()\n",
    "    writer.release()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -O project3-input-video.mp4 https://www.dropbox.com/s/g88o4dx18zpgn8k/projet3-input-video.mp4?dl=1\n",
    "annotate_video(\"project3-input-video.mp4\", \"project3-output-video.mp4\", \"./Frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video = YouTubeVideo(\"\", width=640, height=360)\n",
    "#display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Conclusion</font>\n",
    "\n",
    "The RetinaNet model with the Feature Pyramid Network backbone on top of a feedforward ResNet-## architecture achieved a mAP score of 0.5#.\n",
    "\n",
    "The utility of the [Monk](https://github.com/Tessellate-Imaging/Monk_Object_Detection) framework was questionable. Several blog posts advertise that they build a custom object detector in 5 lines of code. Technically, this is possible. However, one is at the mercy of the framework creators. However, for this project, I rewrote most of the training, validation, testing, and visualization code. Fortunately, I did not have to modify the RetinaNet implementation. Monk has potential, but it needs significant refactoring to allow basic customization, e.g., specifying an optimizer and scheduler, computing the average loss on the validation set to ascertain whether the model is overfitting, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Project 3: Object Detection</font>\n",
    "\n",
    "\n",
    "#### Maximum Points: 100\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Plot Ground Truth Bounding Boxes</h3></td> <td><h3>20</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Training</h3></td> <td><h3>25</h3></td> </tr>\n",
    "        <tr><td><h3>3</h3></td> <td><h3>Inference</h3></td> <td><h3>15</h3></td> </tr>\n",
    "        <tr><td><h3>4</h3></td> <td><h3>COCO Detection Evaluation</h3></td> <td><h3>25</h3></td> </tr>\n",
    "        <tr><td><h3>5</h3></td> <td><h3>Run Inference on a Video</h3></td> <td><h3>15</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:orange\">Project Approach</font>\n",
    "\n",
    "This project fine tunes a pretrained **[TorchVision RetinaNet](https://pytorch.org/vision/0.8/models.html#retinanet)** model using the **Vehicle Registration Plate** dataset. For brevity, a vehicle registration plate is referred to as a license plate. RetinaNet is introducted in the [Focal Loss for Dense Object Detection](https://www.paperswithcode.com/method/retinanet) article and its architecture is depicted below.\n",
    "\n",
    "![RetinaNet network architecture](https://www.paperswithcode.com/media/methods/Screen_Shot_2020-06-07_at_4.22.37_PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:purple\">Download the Dataset</font> \n",
    "\n",
    "**[Download the Vehicle registration plate](https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1)**\n",
    "\n",
    "\n",
    "Download the Vehicle Registration Plate dataset from [here](https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1) and unzip it. \n",
    "\n",
    "We will have the following directory structure:\n",
    "\n",
    "```\n",
    "Dataset\n",
    "├── train\n",
    "│   └── Vehicle registration plate\n",
    "│       └── Label\n",
    "└── validation\n",
    "    └── Vehicle registration plate\n",
    "        └── Label\n",
    "```\n",
    "\n",
    "Unzipping the file will give you a directory `Dataset`. This directory has two folder `train` and `validation`. Each train and validation folder has `Vehicle registration plate`  folder with `.jpg` images and a folder `Labels`.  `Labels` folder has bounding box data for the images.\n",
    "\n",
    "\n",
    "For example,\n",
    "For image: `Dataset/train/Vehicle registration plate/bf4689922cdfd532.jpg`\n",
    "Label file is  `Dataset/train/Vehicle registration plate/Label/bf4689922cdfd532.txt`\n",
    "\n",
    "There are one or more lines in each `.txt` file. Each line represents one bounding box.\n",
    "For example,\n",
    "```\n",
    "Vehicle registration plate 385.28 445.15 618.24 514.225\n",
    "Vehicle registration plate 839.68 266.066462 874.24 289.091462\n",
    "```\n",
    "\n",
    "We have a single class detection (`Vehicle registration plate detection`) problem. So bounding box details start from the fourth column in each row.\n",
    "\n",
    "Representation is in `xmin`, `ymin`, `xmax`, and `ymax` format.\n",
    "\n",
    "**It has `5308` training and `386` validation dataset.**\n",
    "\n",
    "Data is downloaded from [Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Download/Unzip Commands</font>\n",
    "\n",
    "To install the dataset, run the following commands.\n",
    "\n",
    "```\n",
    "wget -O Dataset.zip https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1\n",
    "unzip Dataset.zip\n",
    "rm Dataset.zip\n",
    "mv Dataset/ dataset/\n",
    "```\n",
    "\n",
    "The output should be similar to the following.\n",
    "\n",
    "\n",
    "```\n",
    "--2021-02-10 19:58:37--  https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1\n",
    "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
    "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
    "HTTP request sent, awaiting response... 301 Moved Permanently\n",
    "...\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 1764437533 (1.6G) [application/binary]\n",
    "Saving to: ‘Dataset.zip’\n",
    "\n",
    "Dataset.zip         100%[===================>]   1.64G  25.2MB/s    in 45s     \n",
    "\n",
    "2021-02-10 19:59:24 (37.2 MB/s) - ‘Dataset.zip’ saved [1764437533/1764437533]\n",
    "\n",
    "Archive:  Dataset.zip\n",
    "   creating: Dataset/\n",
    "   creating: Dataset/validation/\n",
    "   creating: Dataset/validation/Vehicle registration plate/\n",
    "  inflating: Dataset/validation/Vehicle registration plate/52ceb1fc30b413e5.jpg  \n",
    "  inflating: Dataset/validation/Vehicle registration plate/182268e1f8c6525f.jpg\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget -O Dataset.zip https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1\n",
    "#! unzip Dataset.zip\n",
    "#! rm Dataset.zip\n",
    "#! mv Dataset/ dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import collections\n",
    "#import json\n",
    "import math\n",
    "import os\n",
    "#import shutil\n",
    "#import sys\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensor, ToTensorV2\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import torchvision\n",
    "#from IPython.utils import io\n",
    "from PIL import Image \n",
    "#from pycocotools.cocoeval import COCOeval\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "runs_dir = \"./runs\"       # the root directory for TensorBoard logs\n",
    "data_dir = \"./dataset\"    # the root directory for the Vehicle Registration Plate dataset\n",
    "model_dir = \"./models\"    # the root directory of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Datastore</font>\n",
    "\n",
    "The `Datastore` class ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datastore(object):\n",
    "    def __init__(self, data_dir):\n",
    "        self.__data_dir = data_dir\n",
    "        self.__train_dir = os.path.join(data_dir, \"train\")\n",
    "        self.__train_image_dir = os.path.join(data_dir, \"train\", \"Vehicle registration plate\")\n",
    "        self.__train_label_dir = os.path.join(data_dir, \"train\", \"Vehicle registration plate\", \"Label\")\n",
    "        self.__valid_dir = os.path.join(data_dir, \"validation\")\n",
    "        self.__valid_image_dir = os.path.join(data_dir, \"validation\", \"Vehicle registration plate\")\n",
    "        self.__valid_label_dir = os.path.join(data_dir, \"validation\", \"Vehicle registration plate\", \"Label\")\n",
    "        if not self.__is_data_set_valid():\n",
    "            raise IOError(f\"'{data_dir}' is not a valid Vehicle Registration Plate dataset.\")\n",
    "        self._train_images = self.__process_data(self.__train_image_dir, self.__train_label_dir)\n",
    "        self._valid_images = self.__process_data(self.__valid_image_dir, self.__valid_label_dir)\n",
    "\n",
    "    def __is_data_set_valid(self):\n",
    "        paths_and_counts = [\n",
    "            (self.__data_dir, 2),\n",
    "            (self.__train_dir, 1),\n",
    "            (self.__train_image_dir, 5309),\n",
    "            (self.__train_label_dir, 5308),\n",
    "            (self.__valid_dir, 1),\n",
    "            (self.__valid_image_dir, 387),\n",
    "            (self.__valid_label_dir, 386)\n",
    "        ]\n",
    "       \n",
    "        for path, count in paths_and_counts:\n",
    "            if not os.path.isdir(path) or len(os.listdir(path)) != count:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def __process_annotations(self, ann_path):\n",
    "        boxes = []\n",
    "        f = open(ann_path, 'r')\n",
    "        for line in f.readlines():\n",
    "            tokens = line.split(' ')\n",
    "            xmin = float(tokens[3])\n",
    "            ymin = float(tokens[4])\n",
    "            xmax = float(tokens[5])\n",
    "            ymax = float(tokens[6])\n",
    "            assert xmax > xmin\n",
    "            assert ymax > ymin\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        return boxes\n",
    "        \n",
    "    def __create_target(self, image_id, boxes):\n",
    "        num_objs = len(boxes)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        return {\n",
    "            \"boxes\": boxes, \n",
    "            \"labels\": torch.zeros((num_objs,), dtype=torch.int64), \n",
    "            \"image_id\": torch.tensor([image_id]), \n",
    "            \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]), \n",
    "            \"iscrowd\": torch.zeros((num_objs,), dtype=torch.int64)            \n",
    "        }\n",
    "        \n",
    "    def __process_data(self, image_dir, label_dir):\n",
    "        images = []\n",
    "        for idx, img_name in enumerate(os.listdir(image_dir)):\n",
    "            image_path = os.path.join(image_dir, img_name)\n",
    "            label_path = os.path.join(label_dir, os.path.splitext(img_name)[0] + \".txt\")           \n",
    "            if os.path.isfile(image_path):\n",
    "                img = Image.open(image_path)\n",
    "                size = img.size\n",
    "                img.close()\n",
    "                del img\n",
    "                images.append({\n",
    "                    \"id\": idx,\n",
    "                    \"path\": image_path,\n",
    "                    \"size\": size,\n",
    "                    \"target\": self.__create_target(idx, self.__process_annotations(label_path))\n",
    "                })               \n",
    "        return images\n",
    "        \n",
    "    def __get_images(self, images, ids):\n",
    "        if ids is None:\n",
    "            return images\n",
    "        return [images[id] for id in ids]\n",
    "        \n",
    "    def get_train_images(self, ids=None):\n",
    "        return self.__get_images(self._train_images, ids)\n",
    "\n",
    "    def get_valid_images(self, ids=None):\n",
    "        return self.__get_images(self._valid_images, ids)\n",
    "\n",
    "    \n",
    "datastore = Datastore(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Dataset Analysis</font>\n",
    "\n",
    "It is useful to understand the dataset's image dimensions when creating transforms. The following cell visualizes the image widths, heights, and aspect ratios as histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_dims():\n",
    "    # create list of training and validation samples\n",
    "    samples = []\n",
    "    samples.extend(datastore.get_train_images())\n",
    "    samples.extend(datastore.get_valid_images())\n",
    "\n",
    "    def get_image_dims(sample):\n",
    "        size = sample[\"size\"]\n",
    "        return (size[0], size[1], float(size[0])/size[1])    \n",
    "\n",
    "    image_dims = tuple(zip(*[get_image_dims(sample) for sample in samples]))\n",
    "    \n",
    "    def plot_histogram(ax, xlabel, data):\n",
    "        min = np.min(data)\n",
    "        max = np.max(data)\n",
    "        ave = np.mean(data)\n",
    "        title = f\"{xlabel} Histogram (Min: {min:.2f}, Max: {max:.2f}, Ave: {ave:.2f})\"\n",
    "        ax.hist(data, facecolor=(1., 0.549, 0))\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.set_title(title)\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1)\n",
    "    fig.set_figwidth(6)\n",
    "    fig.set_figheight(9)\n",
    "\n",
    "    xlabels = (\"Width\", \"Height\", \"Aspect Ratio\")\n",
    "    for ax, xlabel, data in zip(axs, xlabels, image_dims):\n",
    "        plot_histogram(ax, xlabel, data)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    del samples\n",
    "    del image_dims\n",
    "\n",
    "\n",
    "analyze_image_dims()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font style=\"color:green\">1. Plot Ground Truth Bounding Boxes [20 Points]</font> \n",
    "\n",
    "**You have to show three images from validation data with the bounding boxes.**\n",
    "\n",
    "The plotted images should be similar to the following:\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g1.png'>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g2.png'>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g3.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Annotator</font>\n",
    "\n",
    "The `Annotator` class ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotator(object):\n",
    "    @classmethod\n",
    "    def __annotate_image(cls, img_bgr, bboxes, scores=None, color=(255, 140, 0)):\n",
    "        scale = 1\n",
    "        thick = 1\n",
    "        white = (255, 255, 255)\n",
    "        for idx, bbox in enumerate(bboxes):\n",
    "            xminb, yminb, xmaxb, ymaxb = bbox\n",
    "            xminb = int(xminb + 0.5)\n",
    "            yminb = int(yminb + 0.5)\n",
    "            xmaxb = int(xmaxb + 0.5)\n",
    "            ymaxb = int(ymaxb + 0.5)\n",
    "            cv2.rectangle(img_bgr, (xminb, yminb), (xmaxb, ymaxb), color, 5)\n",
    "\n",
    "            text = \"lic. plate\"\n",
    "            if scores is not None:\n",
    "                text = format(scores[idx], \".0%\")\n",
    "            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_DUPLEX, scale, thick)\n",
    "\n",
    "            pad = 8\n",
    "            xmint = xminb\n",
    "            ymint = yminb - text_size[1] - pad\n",
    "            xmaxt = xminb + text_size[0] + pad\n",
    "            ymaxt = yminb\n",
    "            xtext = xmint + int(pad / 2)\n",
    "            ytext = ymint + text_size[1] + int(pad / 2)\n",
    "            cv2.rectangle(img_bgr, (xmint, ymint), (xmaxt, ymaxt), color, 5)\n",
    "            cv2.rectangle(img_bgr, (xmint, ymint), (xmaxt, ymaxt), color, -1)\n",
    "            cv2.putText(img_bgr, text, (xtext, ytext), cv2.FONT_HERSHEY_DUPLEX, scale, white, thick)\n",
    "        return img_bgr\n",
    "    \n",
    "    @classmethod\n",
    "    def annotate_image_datstore(cls, image, color=(255, 140, 0)):\n",
    "        img_bgr = cv2.imread(image[\"path\"])\n",
    "        bboxes = image[\"target\"][\"boxes\"]\n",
    "        return cls.__annotate_image(img_bgr, bboxes, None, color)\n",
    "    \n",
    "    #@classmethod\n",
    "    #def annotate_image_bgr_pred(cls, img_bgr, bboxes, scores, color=(255, 140, 0), vis_threshold = 0.4):\n",
    "    #    mask = scores > vis_threshold\n",
    "    #   return cls.__annotate_image(img_bgr, bboxes[mask], scores[mask], color)\n",
    "    \n",
    "    #@classmethod\n",
    "    #def annotate_image_name_pred(cls, img_name, bboxes, scores, color=(255, 140, 0), vis_threshold = 0.4):\n",
    "    #    img_bgr = cv2.imread(os.path.join(self.__image_dir, img_name))\n",
    "    #    return cls.annotate_image_bgr_pred(img_bgr, bboxes, scores, color, vis_threshold)\n",
    "\n",
    "    @classmethod\n",
    "    def show_image(cls, img):\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "samples = datastore.get_valid_images([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(Annotator.annotate_image_datstore(samples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(Annotator.annotate_image_datstore(samples[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(Annotator.annotate_image_datstore(samples[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">LicensePlateDataset</font>\n",
    "\n",
    "The ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LicensePlateDataset(object):\n",
    "    def __init__(self, datastore, training=False, subset_size=None):\n",
    "        self.__validate_params(training, subset_size)\n",
    "\n",
    "        if self._subset:\n",
    "            self._images = LicensePlateDataset.__create_subset(\n",
    "                self._images, \n",
    "                subset_size\n",
    "            )\n",
    "\n",
    "        self._transforms_A = A.Compose(\n",
    "            self._get_transform_list(),\n",
    "            bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"labels\"])\n",
    "        )\n",
    "\n",
    "        self._transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "        \n",
    "    def __getitem_A__(self, idx):\n",
    "        image = self._images[idx]\n",
    "        target = image[\"target\"]\n",
    "\n",
    "        transformed = self._transforms(\n",
    "            image = skimage.io.imread(image[\"path\"]),\n",
    "            bboxes = target[\"boxes\"],\n",
    "            labels = target[\"labels\"]\n",
    "        )\n",
    "        \n",
    "        #bboxes = torch.round(transformed[\"bboxes\"]).type(torch.IntTensor)\n",
    "        bboxes = transformed[\"bboxes\"]\n",
    "        labels = transformed[\"labels\"]\n",
    "        target = {\"boxes\": bboxes, \"labels\": labels}\n",
    "        return transformed[\"image\"], target\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self._images[idx]\n",
    "        target = image[\"target\"]\n",
    "\n",
    "        image = self._transforms(skimage.io.imread(image[\"path\"]))\n",
    "        \n",
    "        target = {\"boxes\": target[\"boxes\"], \"labels\": target[\"labels\"]}\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._images)\n",
    "\n",
    "    def __validate_params(self, training, subset_size):\n",
    "        self._training = bool(training)\n",
    "        if self._training:\n",
    "            self._images = datastore.get_train_images()\n",
    "        else:\n",
    "            self._images = datastore.get_valid_images()\n",
    "        \n",
    "        self._subset = subset_size is not None\n",
    "        if not self._subset:\n",
    "            self._subset_size = None\n",
    "        else:\n",
    "            self._subset_size = float(subset_size)\n",
    "            if self._subset_size <= 0. or self._subset_size > 1.:\n",
    "                raise ValueError(\"subset_size must be a float whose value is in (0., 1.]\")\n",
    "    \n",
    "    def _get_transform_list(self):\n",
    "        transforms = []\n",
    "        if self._training and not self._subset:\n",
    "            # ToDo: Implement!\n",
    "            pass\n",
    "        transforms.append(ToTensor())\n",
    "        return transforms\n",
    "    \n",
    "    @classmethod\n",
    "    def __create_subset(cls, images, subset_size):\n",
    "        indices = range(len(images))\n",
    "        _, indices = np.unique([int(idx * subset_size) for idx in indices], return_index=True)\n",
    "        return [images[idx] for idx in indices]           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Model Evaluator</font>\n",
    "\n",
    "[Monk](https://github.com/Tessellate-Imaging/Monk_Object_Detection) supplies an `evaluate_coco(dataset, model, threshold=0.05)` function, which evaluates the model against the validation data and prints the COCO metrics. Unfortunately, this function does not return the COCO metrics, so the mAP metric cannot be logged to TensorBoard. Consequently, I modified the aforementioned function to create the `ModelEvaluator` class, which provides the following features.\n",
    "\n",
    "* Suppresses the (print) output from the `pycocotools.cocoeval.COCOeval` support class.\n",
    "* Returns a tuple of the COCO metrics and its primary metric, mean average precision (mAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator(object):\n",
    "    def __init__(self, model, dataset):\n",
    "        self._model = model\n",
    "        self._dataset = dataset\n",
    "        \n",
    "    def Evaluate(self, results_path=None, threshold=0.05):\n",
    "        if results_path is None:\n",
    "            results_path = f\"{self._dataset.set_name}_results.json\"\n",
    "\n",
    "        self._model.eval()\n",
    "        with torch.no_grad():\n",
    "            results = []\n",
    "            image_ids = []\n",
    "            data_len = len(self._dataset)\n",
    "            bar_format = \"{l_bar}{bar}| {n_fmt:>4}/{total_fmt:4} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\"\n",
    "            tqdm_pbar = tqdm(desc=\"CoCoEval\", bar_format=bar_format, total=data_len+1, mininterval=1., unit=\"image\")\n",
    "            for data_idx, data in enumerate(self._dataset):\n",
    "                tqdm_pbar.update(1)\n",
    "                \n",
    "                # run network\n",
    "                scores, labels, bboxes = self._model(data['img'].permute(2, 0, 1).cuda().float().unsqueeze(dim=0))\n",
    "                scores = scores.cpu()\n",
    "                labels = labels.cpu()\n",
    "                bboxes = bboxes.cpu()\n",
    "                \n",
    "                if bboxes.shape[0] > 0:\n",
    "                    # correct boxes for image scale and change to COCO format\n",
    "                    bboxes /= data[\"scale\"]\n",
    "                    bboxes[:, 2] -= bboxes[:, 0]\n",
    "                    bboxes[:, 3] -= bboxes[:, 1]\n",
    "\n",
    "                    for bbox_idx in range(bboxes.shape[0]):\n",
    "                        score = float(scores[bbox_idx])\n",
    "                        label = int(labels[bbox_idx])\n",
    "                        bbox  = bboxes[bbox_idx, :]\n",
    "\n",
    "                        # since scores are sorted, we can jump out of this loop\n",
    "                        # once we reach a score that is smaller than the threshold\n",
    "                        if score < threshold:\n",
    "                            break\n",
    "\n",
    "                        results.append({\n",
    "                            \"image_id\"    : self._dataset.image_ids[data_idx],\n",
    "                            \"category_id\" : self._dataset.label_to_coco_label(label),\n",
    "                            \"score\"       : float(score),\n",
    "                            \"bbox\"        : bbox.tolist(),\n",
    "                        })\n",
    "\n",
    "                # append image to list of processed images\n",
    "                image_ids.append(self._dataset.image_ids[data_idx])\n",
    "\n",
    "            mean_ap = 0.\n",
    "            coco_eval = None\n",
    "            if len(results) > 0:\n",
    "                with io.capture_output() as captured: # surpress output\n",
    "                    # write output\n",
    "                    json.dump(results, open(results_path, 'w'), indent=4)\n",
    "\n",
    "                    # load results in COCO evaluation tool\n",
    "                    coco_true = self._dataset.coco\n",
    "                    coco_pred = coco_true.loadRes(results_path) # why write, then read?\n",
    "\n",
    "                    # run COCO evaluation\n",
    "                    coco_eval = COCOeval(coco_true, coco_pred, \"bbox\")\n",
    "                    coco_eval.params.imgIds = image_ids\n",
    "                    coco_eval.evaluate()\n",
    "                    coco_eval.accumulate()\n",
    "                    coco_eval.summarize()\n",
    "                    mean_ap = coco_eval.stats[0].item()\n",
    "            \n",
    "            tqdm_pbar.set_postfix(mAP=f\"{mean_ap:.03f}\")\n",
    "            tqdm_pbar.update(1)\n",
    "            tqdm_pbar.close()\n",
    "            return coco_eval, mean_ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font style=\"color:green\">2. Training [25 Points]</font> \n",
    "\n",
    "- **Write your training code in this section.**\n",
    "\n",
    "- **You also have to share ([shared logs example](https://tensorboard.dev/experiment/JRtnsKbwTaq1ow6nPLPGeg)) the loss plot of your training using tensorboard.dev.** \n",
    "\n",
    "How to share TensorBoard logs using tensorboard.dev find [here](https://courses.opencv.org/courses/course-v1:OpenCV+OpenCV-106+2019_T1/courseware/b1c43ffe765246658e537109e188addb/d62572ec8bd344db9aeae81235ede618/4?activate_block_id=block-v1%3AOpenCV%2BOpenCV-106%2B2019_T1%2Btype%40vertical%2Bblock%40398b46ddcd5c465fa52cb4d572ba3229)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">\"Refactoring\" Monk Detector Class (via Inheritance)</font>\n",
    "\n",
    "The Monk `Detector` class' `Train` method prints training progress to the cell's output window. This project requires the use of [TensorBoard](https://www.tensorflow.org/tensorboard/). Rather than refactor the Monk framework, I derived a new detector class, `MyDetector`, from Monk's `Detector` class and that adds the following features.\n",
    "\n",
    "* Progress is displayed using `tqdm` progress bars.\n",
    "* Optimizer and LR scheduler may be specified (using `Adam` and `ReduceLROnPlateau`).\n",
    "* Average loss is computed on the validation data each epoch.\n",
    "* The model is saved each time the validation loss is minimized.\n",
    "* Training terminates after N consecutive epochs where the validation loss increases.\n",
    "* The following parameters and metrics are logged to TensorBoard.\n",
    "  * learning rate\n",
    "  * training class loss\n",
    "  * training regression loss\n",
    "  * training total loss\n",
    "  * validation class loss\n",
    "  * validation regression loss\n",
    "  * validation total loss\n",
    "  * validation mAP\n",
    "\n",
    "Monk's data augmentation for RetinaNet is limited. It randomly flips the training data horizontally. If I am not able to achieve the mAP goal of 0.50 with a simpler RetinaNet model that does not overfit the training data, then I will have to implement and/or integrate additional augmentation transforms.\n",
    "\n",
    "Prior to the training loop, the validation loss is computed on the pretrained model. The following operations are performed each epoch.\n",
    "\n",
    "* Training cycle that computes losses on the training data and performs back propogation.\n",
    "* Validation cycle that computes losses on the validation data.\n",
    "* Evaluation cycle that computes mAP on the validation data.\n",
    "* Scheduler update.\n",
    "* Training termination check.\n",
    "\n",
    "Since Monk does not support data subsets, the training pipeline was validated by temporary code that prematurely terminating the training and validation cycles. The `ReduceLROnPlateau` scheduler and the training termination code was tested by temporary code that simulated overfitting by monotonically increasing the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! nvidia-smi\n",
    "! pip list\n",
    "#! pip install numpy\n",
    "#! pip install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cu110/torch_nightly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "dataset = LicensePlateDataset(datastore, False)\n",
    "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
    "model.to(device)\n",
    "image, target = dataset[0]\n",
    "\n",
    "#training\n",
    "images = [image]\n",
    "targets = [target]\n",
    "images = [image.to(device) for image in images]\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "for image in images:\n",
    "    print(f\"image: {image.type()}\")\n",
    "for target in targets:\n",
    "    print(f\"boxes: {target['boxes'].type()}, {target['boxes']}\")\n",
    "    print(f\"labels: {target['labels'].type()}, {target['labels']}\")\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = sched.ReduceLROnPlateau(optimizer, factor=math.sqrt(0.1), patience=3, threshold=0.001)\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "loss_dict = model(image, target)\n",
    "print(\"\\nTraining output\")\n",
    "print(loss_dict.keys())\n",
    "\n",
    "# inferencing\n",
    "images = [image]\n",
    "targets = [target]\n",
    "images = [image.to(device) for image in images]\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "model.eval()\n",
    "preds = model(images)\n",
    "print(\"\\nInferencing output\")\n",
    "print(preds[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inferencer(object):\n",
    "    # ToDo: Implement the following:\n",
    "    # * Compute mAP and other CoCo metrics\n",
    "    # * Interfence a datastore image\n",
    "\n",
    "    # output of model(images) is a list of dictionaries with the following keys.\n",
    "    # * 'boxes'\n",
    "    # * 'scores'\n",
    "    # * 'labels'\n",
    "\n",
    "\n",
    "    def __init__(self, datastore, load_model_name, batch_size=8, num_workers=8, subset_size=None):\n",
    "        # https://pytorch.org/text/stable/datasets.html\n",
    "        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")       \n",
    "        self._model = self._create_model(load_model_name)\n",
    "        self._valid_dataset = LicensePlateDataset(datastore, False, subset_size)\n",
    "        self._valid_dataloader = DataLoader(\n",
    "            dataset = self._valid_dataset,\n",
    "            batch_size = batch_size,\n",
    "            num_workers = num_workers,\n",
    "            shuffle = False\n",
    "        )\n",
    "\n",
    "    def _create_model(self, load_model_name):\n",
    "        # ToDo: Implement\n",
    "        pretrained = load_model_name is None\n",
    "        model = torchvision.models.detection.retinanet_resnet50_fpn(\n",
    "            num_classes = 1, \n",
    "            pretrained_backbone = pretrained\n",
    "        )\n",
    "        if not pretrained:\n",
    "            # https://pytorch.org/docs/stable/generated/torch.load.html\n",
    "            model_path = os.path.join(model_dir, load_model_name)\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "        model.to(self._device)\n",
    "        return model\n",
    "    \n",
    "\n",
    "\n",
    "class Detector(Inferencer):\n",
    "    # ToDo: Implement the following:\n",
    "    # * Train method\n",
    "    \n",
    "    def __init__(self, datastore, load_model_name=None, batch_size=8, num_workers=8, subset_size=None):\n",
    "        super().__init__(datastore, load_model_name, batch_size, num_workers, subset_size)\n",
    "        self._optimizer = None\n",
    "        self._scheduler = None\n",
    "        self._train_dataset = LicensePlateDataset(datastore, True, subset_size)\n",
    "        self._train_dataloader = DataLoader(\n",
    "            dataset = self._train_dataset,\n",
    "            batch_size = batch_size,\n",
    "            num_workers = num_workers,\n",
    "            shuffle = True\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def model_parameters(self):\n",
    "        return self._model.parameters()\n",
    "        \n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        return self._optimizer\n",
    "    \n",
    "    @optimizer.setter\n",
    "    def optimizer(self, value):\n",
    "        self._optimizer = value\n",
    "    \n",
    "    @property\n",
    "    def scheduler(self):\n",
    "        return self._scheduler\n",
    "    \n",
    "    @scheduler.setter\n",
    "    def scheduler(self, value):\n",
    "        self._scheduler = value\n",
    "    \n",
    "    def train(\n",
    "        self, \n",
    "        exper_name, \n",
    "        num_epochs, \n",
    "        start_epoch=0,\n",
    "        no_best_thd=0\n",
    "    ):\n",
    "        model = self._model\n",
    "        device = self._device\n",
    "        optimizer = self._optimizer\n",
    "        scheduler = self._scheduler\n",
    "        train_dataloader = self._train_dataloader\n",
    "        valid_dataloader = self._valid_dataloader\n",
    "\n",
    "        def create_tqdm_iter(desc, dataloader):\n",
    "            bar_format = \"{l_bar}{bar}| {n_fmt:>4}/{total_fmt:4} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\"\n",
    "            return tqdm(dataloader, bar_format=bar_format, desc=desc, mininterval=1., unit=\"batch\")\n",
    "\n",
    "        def train_cycle(tqdm_desc, dataloader):\n",
    "            model.train()\n",
    "\n",
    "            cstats = self.SimpleStatistics()\n",
    "            rstats = self.SimpleStatistics()\n",
    "            tstats = self.SimpleStatistics()\n",
    "            tqdm_dataloader = create_tqdm_iter(tqdm_desc, dataloader)\n",
    "            for iter_num, (images, targets) in enumerate(tqdm_dataloader):\n",
    "                optimizer.zero_grad()\n",
    "                output = model(images.to(device), targets.to(device))\n",
    "                closs = output[\"classification\"]\n",
    "                rloss = output[\"bbox_regression\"]\n",
    "                \n",
    "                closs = closs.mean()\n",
    "                rloss = rloss.mean()\n",
    "                tloss = closs + rloss\n",
    "\n",
    "                tloss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                cstats.add(float(closs))\n",
    "                rstats.add(float(rloss))\n",
    "                tstats.add(float(tloss))\n",
    "                loss_text = f\"{{C:{cstats.mean:.3f}, R:{rstats.mean:.3f}, T:{tstats.mean:.3f}}}\"\n",
    "                tqdm_dataloader.set_postfix(losses=loss_text)\n",
    "\n",
    "                del closs\n",
    "                del rloss\n",
    "\n",
    "            tqdm_dataloader.close()\n",
    "            return cstats.mean, rstats.mean, tstats.mean\n",
    "\n",
    "        def valid_cycle(tqdm_desc, dataloader):\n",
    "            # Note: If model is set to evaluation mode, then it returns scores, labels and bboxes.\n",
    "            #       We want classification and regression loss, so we set it to training mode and\n",
    "            #       use torch.no_grad() to avoid gradient computation.\n",
    "            model.train()\n",
    "\n",
    "            cstats = self.SimpleStatistics()\n",
    "            rstats = self.SimpleStatistics()\n",
    "            tstats = self.SimpleStatistics()\n",
    "            tqdm_dataloader = create_tqdm_iter(tqdm_desc, dataloader)\n",
    "            for iter_num, data in enumerate(tqdm_dataloader):\n",
    "                with torch.no_grad():        \n",
    "                    output = model(images.to(device), targets.to(device))\n",
    "                    closs = output[\"classification\"]\n",
    "                    rloss = output[\"bbox_regression\"]\n",
    "\n",
    "                closs = closs.mean()\n",
    "                rloss = rloss.mean()\n",
    "                tloss = closs + rloss\n",
    "\n",
    "                cstats.add(float(closs))\n",
    "                rstats.add(float(rloss))\n",
    "                tstats.add(float(tloss))\n",
    "                loss_text = f\"{{C:{cstats.mean:.3f}, R:{rstats.mean:.3f}, T:{tstats.mean:.3f}}}\"\n",
    "                tqdm_dataloader.set_postfix(losses=loss_text)\n",
    "\n",
    "                del closs\n",
    "                del rloss\n",
    "\n",
    "            tqdm_dataloader.close()\n",
    "            return cstats.mean, rstats.mean, tstats.mean        \n",
    "            \n",
    "        min_valid_loss = 1e10\n",
    "        epochs_since_best = 0\n",
    "        save_model_name = exper_name + \".pt\"\n",
    "        writer = SummaryWriter(log_dir=os.path.join(runs_dir, exper_name))\n",
    "        for epoch_num in range(start_epoch, num_epochs):\n",
    "\n",
    "            train_closs, train_rloss, train_tloss = train_cycle(\n",
    "                f\"Train {epoch_num:02d}\", \n",
    "                train_dataloader\n",
    "            )\n",
    "            writer.add_scalar(\"training/class_loss\", train_closs, epoch_num)\n",
    "            writer.add_scalar(\"training/regression_loss\", train_rloss, epoch_num)\n",
    "            writer.add_scalar(\"training/total_loss\", train_tloss, epoch_num)\n",
    "            \n",
    "            valid_closs, valid_rloss, valid_tloss = valid_cycle(\n",
    "                f\"Valid {epoch_num:02d}\", \n",
    "                valid_dataloader\n",
    "            )\n",
    "            writer.add_scalar(\"validation/class_loss\", valid_closs, epoch_num)\n",
    "            writer.add_scalar(\"validation/regression_loss\", valid_rloss, epoch_num)\n",
    "            writer.add_scalar(\"validation/total_loss\", valid_tloss, epoch_num)\n",
    "            \n",
    "            coco_eval, mean_ap = self.ModelEvaluator.Evaluate()\n",
    "            writer.add_scalar(\"test/mAP\", mean_ap, epoch_num)\n",
    "\n",
    "            writer.add_scalar(\"learning_rate\", optimizer.param_groups[0]['lr'], epoch_num)\n",
    "            \n",
    "            if isinstance(scheduler, sched.ReduceLROnPlateau):\n",
    "                scheduler.step(valid_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "            if min_valid_loss > valid_loss:\n",
    "                epochs_since_best = 0\n",
    "                min_valid_loss = valid_loss\n",
    "                self._save_model(save_model_name)\n",
    "            else:\n",
    "                epochs_since_best += 1\n",
    "                if no_best_thd is not None and epochs_since_best >= no_best_thd:\n",
    "                    break\n",
    "        \n",
    "        writer.close()\n",
    "        model.eval()\n",
    "    \n",
    "    def _save_model(save_model_name):\n",
    "        # https://pytorch.org/docs/stable/generated/torch.save.html\n",
    "        model_path = os.path.join(model_dir, save_model_name)\n",
    "        self._model.to(torch.device(\"cpu\"))\n",
    "        torch.save(self._model.state_dict(), model_path)        \n",
    "        self._model.to(self._device)\n",
    "\n",
    "    class SimpleStatistics(object):\n",
    "        def __init__(self):\n",
    "            self._n = 0\n",
    "            self._total = 0.\n",
    "\n",
    "        @property\n",
    "        def n(self):\n",
    "            return self._n\n",
    "\n",
    "        @property\n",
    "        def mean(self):\n",
    "            if self._n == 0:\n",
    "                return 0.\n",
    "            return self._total / self._n\n",
    "\n",
    "        def add(self, x):\n",
    "            self._n += 1\n",
    "            self._total += x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Training Experiments</font>\n",
    "\n",
    "Training experiments are conducted via the `conduct_experiment(model_name, exper_name)` function. The `model_name` parameter may be one of the following values. The `exper_name` parameter is the experiment name. The name of the saved model is the experiment name with a .pt extension.\n",
    "\n",
    "* resnet18\n",
    "* resnet34\n",
    "* resnet50\n",
    "* resnet101\n",
    "* resnet152\n",
    "\n",
    "TQDM progress bars are used to display training progress. The following output is the first three epochs of training the resnet18 model.\n",
    "\n",
    "```\n",
    "Valid 00: 100%|██████████|   39/39   [00:23<00:00,  1.66batch/s, losses={C:1.034, R:0.979, T:2.012}]\n",
    "Train 01: 100%|██████████|  531/531  [12:12<00:00,  1.38s/batch, losses={C:0.315, R:0.508, T:0.823}]\n",
    "Valid 01: 100%|██████████|   39/39   [00:23<00:00,  1.69batch/s, losses={C:0.290, R:0.426, T:0.716}]\n",
    "CoCoEval: 100%|██████████|  387/387  [01:04<00:00,  5.99image/s, mAP=0.352]\n",
    "Train 02: 100%|██████████|  531/531  [12:12<00:00,  1.38s/batch, losses={C:0.174, R:0.333, T:0.507}]\n",
    "Valid 02: 100%|██████████|   39/39   [00:23<00:00,  1.67batch/s, losses={C:0.181, R:0.328, T:0.509}]\n",
    "CoCoEval: 100%|██████████|  387/387  [01:04<00:00,  5.97image/s, mAP=0.431]\n",
    "Train 03: 100%|██████████|  531/531  [12:12<00:00,  1.38s/batch, losses={C:0.132, R:0.282, T:0.415}]\n",
    "Valid 03: 100%|██████████|   39/39   [00:22<00:00,  1.72batch/s, losses={C:0.203, R:0.298, T:0.501}]\n",
    "CoCoEval: 100%|██████████|  387/387  [01:04<00:00,  5.98image/s, mAP=0.447]\n",
    "```\n",
    "\n",
    "The TensorBoard logs may be viewed by clicking this [link]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiment(exper_name):\n",
    "    detector = Detector(datastore)\n",
    "    detector.optimizer = optim.Adam(detector.model_parameters, lr=0.0001)\n",
    "    detector.scheduler = sched.ReduceLROnPlateau(detector.optimizer, factor=math.sqrt(0.1), patience=3, threshold=0.001)\n",
    "    detector.train(num_epochs=1, exper_name=exper_name)   \n",
    "    del detector\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# uncomment the following lines to run an experiment\n",
    "conduct_experiment(\"RetinaNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font style=\"color:green\">3. Inference [15 Points]</font> \n",
    "\n",
    "**You have to make predictions from your trained model on three images from the validation dataset.**\n",
    "\n",
    "The plotted images should be similar to the following:\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p1.png'>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p2.png'>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p3.png'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">\"Refactoring\" Monk Inference Class (via Inheritance)</font>\n",
    "\n",
    "The Monk `Infer` class' Predict method only accepts a path to an image. It also does not annotate images as nicely as my `Annotator`.  Rather than refactor the Monk framework, I derived a new inference class, `MyInferencer`, from Monk's `Infer` class that adds the following features.\n",
    "\n",
    "* Predictions may be performed on RBG and BGR in-memory images.\n",
    "* Predictions and result annotation may be performed on validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyInferencer(Infer):\n",
    "    def __init__(self, image_dir, annotator, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.system_dict[\"dataset\"] = {};\n",
    "        self.system_dict[\"dataset\"][\"val\"] = {};\n",
    "        self.__image_dir = image_dir\n",
    "        self.__annotator = annotator\n",
    "        self.__model_evaluator = None\n",
    "        \n",
    "    @property\n",
    "    def ModelEvaluator(self):\n",
    "        if self.__model_evaluator is None:\n",
    "            self.__model_evaluator = ModelEvaluator(\n",
    "                self.system_dict[\"local\"][\"model\"],\n",
    "                self.system_dict[\"local\"][\"dataset_val\"]\n",
    "            )\n",
    "        return self.__model_evaluator\n",
    "\n",
    "    def Val_Dataset(self, root_dir, coco_dir, img_dir, set_dir):\n",
    "        self.system_dict[\"dataset\"][\"val\"][\"status\"] = True;\n",
    "        self.system_dict[\"dataset\"][\"val\"][\"root_dir\"] = root_dir;\n",
    "        self.system_dict[\"dataset\"][\"val\"][\"coco_dir\"] = coco_dir;\n",
    "        self.system_dict[\"dataset\"][\"val\"][\"img_dir\"] = img_dir;\n",
    "        self.system_dict[\"dataset\"][\"val\"][\"set_dir\"] = set_dir;  \n",
    "\n",
    "        self.system_dict[\"local\"][\"dataset_val\"] = CocoDataset(\n",
    "            self.system_dict[\"dataset\"][\"val\"][\"root_dir\"] + \"/\" + self.system_dict[\"dataset\"][\"val\"][\"coco_dir\"], \n",
    "            img_dir = self.system_dict[\"dataset\"][\"val\"][\"img_dir\"], \n",
    "            set_dir = self.system_dict[\"dataset\"][\"val\"][\"set_dir\"],\n",
    "            transform = transforms.Compose([Normalizer(), Resizer()])\n",
    "        )\n",
    "\n",
    "    def PredictViaImageRGB(self, img):\n",
    "        # normalize image\n",
    "        image = img.astype(np.float32) / 255.;\n",
    "        image = (image.astype(np.float32) - self.system_dict[\"local\"][\"mean\"]) / self.system_dict[\"local\"][\"std\"];\n",
    "       \n",
    "        # compute scaling factor\n",
    "        rows, cols, cns = image.shape\n",
    "        smallest_side = min(rows, cols)\n",
    "        scale = self.system_dict[\"local\"][\"min_side\"] / smallest_side\n",
    "        largest_side = max(rows, cols)\n",
    "        if largest_side * scale > self.system_dict[\"local\"][\"max_side\"]:\n",
    "            scale = self.system_dict[\"local\"][\"max_side\"]  / largest_side\n",
    "\n",
    "        # resize and pad the image\n",
    "        image = skimage.transform.resize(image, (int(round(rows*scale)), int(round((cols*scale)))))\n",
    "        rows, cols, cns = image.shape\n",
    "        pad_w = 32 - rows%32\n",
    "        pad_h = 32 - cols%32\n",
    "        new_image = np.zeros((rows + pad_w, cols + pad_h, cns)).astype(np.float32)\n",
    "        new_image[:rows, :cols, :] = image.astype(np.float32)\n",
    "\n",
    "        # convert image to tensor and perform prediction\n",
    "        img = torch.from_numpy(new_image)\n",
    "        with torch.no_grad():\n",
    "            model = self.system_dict[\"local\"][\"model\"]\n",
    "            scores, labels, boxes = model(img.cuda().permute(2, 0, 1).float().unsqueeze(dim=0))\n",
    "            boxes /= scale\n",
    "\n",
    "        return scores, labels, boxes\n",
    "        \n",
    "    def PredictViaImageBGR(self, img):\n",
    "        return self.PredictViaImageRGB(img[:, :, ::-1])\n",
    "        \n",
    "    def PredictViaImageName(self, img_name):\n",
    "        img_path = os.path.join(self.__image_dir, img_name)\n",
    "        return self.PredictViaImageRGB(skimage.io.imread(img_path))\n",
    "    \n",
    "    def PredictAndAnnotateSample(self, sample, annotator, vis_threshold=0.4):\n",
    "        img, _ = sample\n",
    "        img_name = img[\"file_name\"]\n",
    "        scores, _, bboxes = self.PredictViaImageName(img_name)\n",
    "        return self.__annotator.annotate_image_name_pred(\n",
    "            img_name = img_name, \n",
    "            bboxes = bboxes, \n",
    "            scores = scores, \n",
    "            vis_threshold = vis_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Detecting and Annotating License Plates in Sample Validation Images</font>\n",
    "\n",
    "The following cell creates an \"inferencer\", loads the trained model, and annotates predictions on the same three samples used in Section 1 to plot ground truth bounding boxes. The subsequent three cells perform a prediction on each sample and annotate the detected license plates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exper_name = \"RetinaNet18\"\n",
    "inferencer = MyInferencer(dataset_helper.valid_image_dir, Annotator(dataset_helper.valid_image_dir))\n",
    "inferencer.Model(model_path=f\"{exper_name}.pt\")\n",
    "samples = dataset_helper.get_valid_samples([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(inferencer.PredictAndAnnotateSample(samples[0], annotator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(inferencer.PredictAndAnnotateSample(samples[1], annotator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(inferencer.PredictAndAnnotateSample(samples[2], annotator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font style=\"color:green\">4. COCO Detection Evaluation [25 Points]</font> \n",
    "\n",
    "**You have to evaluate your detection model on COCO detection evaluation metric.**\n",
    "\n",
    "For your reference here is the coco evaluation metric chart:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/03/c3-w9-coco_metric.png\">\n",
    "\n",
    "---\n",
    "\n",
    "#### <font style=\"color:red\">The expected `AP` (primary challenge metric) is more than `0.5`.</font>\n",
    "\n",
    "**The expected output should look similar to the following:**\n",
    "\n",
    "```\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.550\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.886\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.629\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.256\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.653\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.627\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.504\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.629\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.633\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.380\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.722\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.704\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Detection Model Evaluation</font>\n",
    "\n",
    "The following cell initializes the inferencer's validation dataset and requests its `ModelEvaluator`. It then calls the model evaluator's `Evaluate` method and summarizes the resulting CoCo evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.capture_output() as captured: # surpress output\n",
    "    inferencer.Val_Dataset(\n",
    "        root_dir = data_dir,          # path to root directory containing coco_dir\n",
    "        coco_dir = \"license_plates\",  # name of coco_dir contianing image and annotation folders\n",
    "        img_dir = \"./\",               # name of folder containing all training and validation folders\n",
    "        set_dir = \"images_valid\"\n",
    "    )\n",
    "\n",
    "evaluator = inferencer.ModelEvaluator\n",
    "coco_eval, _ = evaluator.Evaluate()   \n",
    "coco_eval.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:green\">5. Run Inference on a Video [15 Points]</font>\n",
    "\n",
    "#### [Download the Input Video](https://www.dropbox.com/s/g88o4dx18zpgn8k/projet3-input-video.mp4?dl=1)\n",
    "\n",
    "**You have to run inference on a video.** \n",
    "\n",
    "You can download the video from [here](https://www.dropbox.com/s/g88o4dx18zpgn8k/projet3-input-video.mp4?dl=1).\n",
    "\n",
    "#### <font style=\"color:red\">Upload the output video on youtube and share the link. Do not upload the video in the lab.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo, display\n",
    "video = YouTubeVideo(\"18HWHCevFdU\", width=640, height=360)\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your output video should have a bounding box around the vehicle registration plate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = YouTubeVideo(\"5SgCuee7AMs\", width=640, height=360)\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Detection and Annotating a Video</font>\n",
    "\n",
    "I modified the sample function to read the source video frame-by-frame, detect and annotate license plates in each frame, and create an output video. I uploaded this video to YouTube and it may be viewed in this notebook's last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_video(src_video_path, dst_video_path, frames_dir=None):\n",
    "    \"\"\"\n",
    "    Create a new video by annotating license plates in the source video.\n",
    "    \n",
    "    src_video_path (str): path of source video in which to annotate\n",
    "    dst_video_path (str): path to destination video in which to create\n",
    "    frames_dir optional(str): if specified, direction to write annotated frames\n",
    "    \"\"\"\n",
    "    \n",
    "    # create a video reader\n",
    "    reader = cv2.VideoCapture(src_video_path)\n",
    "    if not reader.isOpened(): \n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "        \n",
    "    # get source video attributes\n",
    "    width = int(reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frames_per_second = float(reader.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # create a video writer\n",
    "    writer = cv2.VideoWriter(\n",
    "        filename = dst_video_path,\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "        fps =  frames_per_second,\n",
    "        frameSize = (width, height),\n",
    "        isColor=True,\n",
    "    )\n",
    "    \n",
    "    # for each frame in the source video ...\n",
    "    count = 0\n",
    "    while reader.isOpened():\n",
    "        # read the frame\n",
    "        ret, frame = reader.read()\n",
    "        if ret:\n",
    "            # detect and annotate license plates\n",
    "            scores, _, bboxes = inferencer.PredictViaImageBGR(frame)\n",
    "            frame = annotator.annotate_image_bgr_pred(frame, bboxes, scores, color=(0, 140, 255))\n",
    "            # write frame to destination video\n",
    "            writer.write(frame)\n",
    "            # if specified, write frame to frames directory\n",
    "            if frames_dir is not None:\n",
    "                name = f\"frame{count:04d}.jpg\"\n",
    "                path = os.path.join(frames_dir, name)\n",
    "                cv2.imwrite(path, frame)\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # free reader and writer resources\n",
    "    reader.release()\n",
    "    writer.release()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -O project3-input-video.mp4 https://www.dropbox.com/s/g88o4dx18zpgn8k/projet3-input-video.mp4?dl=1\n",
    "annotate_video(\"project3-input-video.mp4\", \"project3-output-video.mp4\", \"./Frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video = YouTubeVideo(\"\", width=640, height=360)\n",
    "#display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Conclusion</font>\n",
    "\n",
    "The RetinaNet model with the Feature Pyramid Network backbone on top of a feedforward ResNet-## architecture achieved a mAP score of 0.5#.\n",
    "\n",
    "The utility of the [Monk](https://github.com/Tessellate-Imaging/Monk_Object_Detection) framework was questionable. Several blog posts advertise that they build a custom object detector in 5 lines of code. Technically, this is possible. However, one is at the mercy of the framework creators. However, for this project, I rewrote most of the training, validation, testing, and visualization code. Fortunately, I did not have to modify the RetinaNet implementation. Monk has potential, but it needs significant refactoring to allow basic customization, e.g., specifying an optimizer and scheduler, computing the average loss on the validation set to ascertain whether the model is overfitting, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

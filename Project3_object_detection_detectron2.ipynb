{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Project 3: Object Detection</font>\n",
    "\n",
    "\n",
    "#### Maximum Points: 100\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Plot Ground Truth Bounding Boxes</h3></td> <td><h3>20</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Training</h3></td> <td><h3>25</h3></td> </tr>\n",
    "        <tr><td><h3>3</h3></td> <td><h3>Inference</h3></td> <td><h3>15</h3></td> </tr>\n",
    "        <tr><td><h3>4</h3></td> <td><h3>COCO Detection Evaluation</h3></td> <td><h3>25</h3></td> </tr>\n",
    "        <tr><td><h3>5</h3></td> <td><h3>Run Inference on a Video</h3></td> <td><h3>15</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:orange\">Project Approach</font>\n",
    "\n",
    "In the last project I created a framework by which I could easily create, run, and analyze experiments. I choose this approach in order to learn how to program in Python. In this project I will use an existing framework to reduce my development effort.\n",
    "\n",
    "After reading the article **[RetinaNet: Custom Object Detection training with 5 lines of code](https://laptrinhx.com/retinanet-custom-object-detection-training-with-5-lines-of-code-1882442374/)**, I started prototyping with [Monk](https://github.com/Tessellate-Imaging/Monk_Object_Detection)'s [RetinaNet](https://www.paperswithcode.com/method/retinanet) implementation (https://www.paperswithcode.com/method/retinanet), which is built upon [PyTorch RetinaNet](https://github.com/yhenon/pytorch-retinanet). I abandoned this framework after realizing the following.\n",
    "\n",
    "* Monk's documentation was very limited.\n",
    "* Monk's RetinaNet implementation was not actively maintained and had many outstanting issues.\n",
    "* Substantial refactoring and development were required to meet the project's requirements.\n",
    "\n",
    "I ultimately decided to use [FaceBook AI](https://ai.facebook.com/)'s [Detectron2](https://github.com/facebookresearch/detectron2). Its introductory blog post, [Detectron2: A PyTorch-based modular object detection library](https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/), describes this framework as follows.\n",
    "\n",
    "> Detectron2 is a ground-up rewrite of Detectron that started with \n",
    "[maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark). The platform is now implemented in \n",
    "[PyTorch](https://pytorch.org/). With a new, more modular design, Detectron2 is flexible and extensible, and able to provide fast training on single or multiple GPU servers. Detectron2 includes high-quality implementations of state-of-the-art object detection algorithms, including [DensePose](http://densepose.org/), [panoptic feature pyramid networks](https://ai.facebook.com/blog/improving-scene-understanding-through-panoptic-segmentation/), and numerous variants of the pioneering [Mask R-CNN](https://research.fb.com/publications/mask-r-cnn/) model family also developed by FAIR. Its extensible design makes it easy to implement cutting-edge research projects without having to fork the [entire codebase](https://github.com/facebookresearch/detectron2).\n",
    "\n",
    "This project fine-tunes pretrained [Detectron2 RetinaNet](https://detectron2.readthedocs.io/en/latest/modules/modeling.html?highlight=retinanet#detectron2.modeling.RetinaNet) models using the class supplied _Vehicle Registration Plate_ dataset. For brevity, a vehicle registration plate is referred to as a license plate. RetinaNet is introducted in the **[Focal Loss for Dense Object Detection](https://www.paperswithcode.com/method/retinanet)** article and its architecture is depicted below.\n",
    "\n",
    "![RetinaNet network architecture](https://www.paperswithcode.com/media/methods/Screen_Shot_2020-06-07_at_4.22.37_PM.png)\n",
    "\n",
    "RetinaNet is a one-stage object detection model that utilizes a focal loss function to address class imbalance during training. Focal loss applies a modulating term to the cross entropy loss in order to focus learning on hard negative examples. The impact of this modulating term is depicted for various value of ɣ in the plots below.\n",
    "\n",
    "![Focal loss plot](https://www.starlg.cn/2019/01/10/Focal-Loss/focalLoss.png)\n",
    "\n",
    "Detectron2 provides trainers, inferencers, model evaluators, and visualization tools. Nevertheless, I decided to implement my own versions of these components for the following reasons.\n",
    "\n",
    "* To understand the implementation details involved in object detection.\n",
    "* To understand how validation loss correlates with object detection evaluation metrics, e.g., mAP.\n",
    "* To learn how to annotate images in Python on a simply problem, drawing labeled bounding boxes.\n",
    "* The output of the Detectron2's components is rather untidy.\n",
    "\n",
    "**Note:** Since the Detectron2 framework had other pretrained object detector models, this project compared RetinaNet-50 and RetinaNet-101 to faster R-CNN-X101."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:purple\">Download the Dataset</font> \n",
    "\n",
    "**[Download the Vehicle registration plate](https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1)**\n",
    "\n",
    "\n",
    "Download the Vehicle Registration Plate dataset from [here](https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1) and unzip it. \n",
    "\n",
    "We will have the following directory structure:\n",
    "\n",
    "```\n",
    "Dataset\n",
    "├── train\n",
    "│   └── Vehicle registration plate\n",
    "│       └── Label\n",
    "└── validation\n",
    "    └── Vehicle registration plate\n",
    "        └── Label\n",
    "```\n",
    "\n",
    "Unzipping the file will give you a directory `Dataset`. This directory has two folders: `train` and `validation`. Each train and validation folder has a `Vehicle registration plate`  folder with `.jpg` images and a folder `Labels` with `.txt` files. The `Label` folders have bounding box data for the images.\n",
    "\n",
    "\n",
    "For example, the image, `Dataset/train/Vehicle registration plate/bf4689922cdfd532.jpg`, has the corresponding label file, `Dataset/train/Vehicle registration plate/Label/bf4689922cdfd532.txt`.\n",
    "\n",
    "There are one or more lines in each `.txt` file. Each line represents a bounding box in the `class`, `xmin`, `ymin`, `xmax`, and `ymax` format. A representative label file is shown below.\n",
    "\n",
    "```\n",
    "Vehicle registration plate 385.28 445.15 618.24 514.225\n",
    "Vehicle registration plate 839.68 266.066462 874.24 289.091462\n",
    "```\n",
    "\n",
    "This dataset has **5308** training and **386** validation dataset.\n",
    "\n",
    "Data is downloaded from [Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Download/Unzip Commands</font>\n",
    "\n",
    "To install the dataset, run the following commands.\n",
    "\n",
    "```\n",
    "wget -O Dataset.zip https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1\n",
    "unzip Dataset.zip\n",
    "rm Dataset.zip\n",
    "mv Dataset/ dataset/\n",
    "```\n",
    "\n",
    "The output should be similar to the following.\n",
    "\n",
    "\n",
    "```\n",
    "--2021-02-10 19:58:37--  https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1\n",
    "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
    "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
    "HTTP request sent, awaiting response... 301 Moved Permanently\n",
    "...\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 1764437533 (1.6G) [application/binary]\n",
    "Saving to: ‘Dataset.zip’\n",
    "\n",
    "Dataset.zip         100%[===================>]   1.64G  25.2MB/s    in 45s     \n",
    "\n",
    "2021-02-10 19:59:24 (37.2 MB/s) - ‘Dataset.zip’ saved [1764437533/1764437533]\n",
    "\n",
    "Archive:  Dataset.zip\n",
    "   creating: Dataset/\n",
    "   creating: Dataset/validation/\n",
    "   creating: Dataset/validation/Vehicle registration plate/\n",
    "  inflating: Dataset/validation/Vehicle registration plate/52ceb1fc30b413e5.jpg  \n",
    "  inflating: Dataset/validation/Vehicle registration plate/182268e1f8c6525f.jpg\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Uncomment the following lines to download and unzip the dataset\n",
    "\n",
    "#! wget -O Dataset.zip https://www.dropbox.com/s/k81ljpmzy3fgtx9/Dataset.zip?dl=1\n",
    "#! unzip Dataset.zip\n",
    "#! rm Dataset.zip\n",
    "#! mv Dataset/ dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: For convenience, I added detectron2 to a Docker image along with Python, Pytorch GPU, Jupyter notebooks, etc.\n",
    "#       Other who try to run this notebook must install detectron2 and other dependencies via pip from this notebook.\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import detectron2\n",
    "import detectron2.checkpoint\n",
    "import detectron2.config\n",
    "import detectron2.data\n",
    "import detectron2.data.detection_utils as utils\n",
    "import detectron2.evaluation\n",
    "import detectron2.model_zoo\n",
    "import detectron2.solver\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import torch\n",
    "from detectron2.data import transforms as T\n",
    "from IPython.utils import io\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class classproperty(property):\n",
    "    def __get__(self, cls, owner):\n",
    "        return classmethod(self.fget).__get__(None, owner)()\n",
    "\n",
    "    \n",
    "# ToDo: Investigate the warning below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Vehicle Registration Plate Datastore</font>\n",
    "\n",
    "To use custom datasets with Detectron2 for standard tasks, e.g., instance detection, instance/semantic/panoptic segmentation, keypoint detection, the dataset must be converted to Detectron2's [standard representation](https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html#standard-dataset-dicts). The `Datastore` class follows the [singleton design pattern](https://www.tutorialspoint.com/python_design_patterns/python_design_patterns_singleton.htm). It loads this project's custom dataset into `list[dict]`. It also provides a method to fetch 1) specific image dicts via a list of image identifiers, or 2) a subset of image dicts via a percentage in the range of (0., 1.]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datastore(object):\n",
    "    __instance = None\n",
    "    __data_dir = \"./dataset\"\n",
    "    \n",
    "    @classproperty\n",
    "    def instance(cls):\n",
    "        if Datastore.__instance == None:\n",
    "            Datastore()\n",
    "        return Datastore.__instance\n",
    "\n",
    "    @classproperty\n",
    "    def data_dir(cls):\n",
    "        return __data_dir\n",
    "\n",
    "    def __init__(self):\n",
    "        if Datastore.__instance != None:\n",
    "            raise Exception(\"Datastore is a singleton. Use the Datastore.get_instance() method.\")\n",
    "        else:\n",
    "            Datastore.__instance = self\n",
    "            data_dir = Datastore.__data_dir\n",
    "            self.__data_dir = Datastore.__data_dir\n",
    "            self.__train_dir = os.path.join(data_dir, \"train\")\n",
    "            self.__train_image_dir = os.path.join(data_dir, \"train\", \"Vehicle registration plate\")\n",
    "            self.__train_label_dir = os.path.join(data_dir, \"train\", \"Vehicle registration plate\", \"Label\")\n",
    "            self.__valid_dir = os.path.join(data_dir, \"validation\")\n",
    "            self.__valid_image_dir = os.path.join(data_dir, \"validation\", \"Vehicle registration plate\")\n",
    "            self.__valid_label_dir = os.path.join(data_dir, \"validation\", \"Vehicle registration plate\", \"Label\")\n",
    "            if not self.__is_data_set_valid():\n",
    "                raise IOError(f\"'{data_dir}' is not a valid Vehicle Registration Plate dataset.\")\n",
    "            self._train_images = self.__process_data(self.__train_image_dir, self.__train_label_dir)\n",
    "            self._valid_images = self.__process_data(self.__valid_image_dir, self.__valid_label_dir)\n",
    "\n",
    "    def __is_data_set_valid(self):\n",
    "        paths_and_counts = [\n",
    "            (self.__data_dir, 2),\n",
    "            (self.__train_dir, 1),\n",
    "            (self.__train_image_dir, 5309),\n",
    "            (self.__train_label_dir, 5308),\n",
    "            (self.__valid_dir, 1),\n",
    "            (self.__valid_image_dir, 387),\n",
    "            (self.__valid_label_dir, 386)\n",
    "        ]\n",
    "       \n",
    "        for path, count in paths_and_counts:\n",
    "            if not os.path.isdir(path) or len(os.listdir(path)) != count:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def __process_annotations(self, ann_path):\n",
    "        boxes = []\n",
    "        f = open(ann_path, 'r')\n",
    "        for line in f.readlines():\n",
    "            tokens = line.split(' ')\n",
    "            xmin = float(tokens[3])\n",
    "            ymin = float(tokens[4])\n",
    "            xmax = float(tokens[5])\n",
    "            ymax = float(tokens[6])\n",
    "            assert xmax > xmin\n",
    "            assert ymax > ymin\n",
    "            boxes.append({\n",
    "                \"bbox\": [xmin, ymin, xmax, ymax],\n",
    "                \"bbox_mode\": detectron2.structures.BoxMode.XYXY_ABS,\n",
    "                \"category_id\": 0,\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "        return boxes\n",
    "        \n",
    "    def __process_data(self, image_dir, label_dir):\n",
    "        images = []\n",
    "        for idx, img_name in enumerate(os.listdir(image_dir)):\n",
    "            image_path = os.path.join(image_dir, img_name)\n",
    "            label_path = os.path.join(label_dir, os.path.splitext(img_name)[0] + \".txt\")           \n",
    "            if os.path.isfile(image_path):\n",
    "                img = Image.open(image_path)\n",
    "                size = img.size\n",
    "                img.close()\n",
    "                del img\n",
    "                images.append({\n",
    "                    \"file_name\": image_path,\n",
    "                    \"image_id\": idx,\n",
    "                    \"height\": size[1],\n",
    "                    \"width\": size[0],\n",
    "                    \"annotations\": self.__process_annotations(label_path)\n",
    "                })               \n",
    "        return images\n",
    "        \n",
    "    def get_train_image_dict_list(self, ids=None, subset_size=None):\n",
    "        return Datastore.__get_image_dict_list(self._train_images, ids, subset_size)\n",
    "\n",
    "    def get_valid_image_dict_list(self, ids=None, subset_size=None):\n",
    "        return Datastore.__get_image_dict_list(self._valid_images, ids, subset_size)\n",
    "    \n",
    "    @classmethod\n",
    "    def __get_image_dict_list(cls, images, ids, subset_size):\n",
    "        if ids is not None:\n",
    "            return [images[id] for id in ids]\n",
    "        \n",
    "        if subset_size is not None:\n",
    "            subset_size = float(subset_size)\n",
    "            if subset_size <= 0. or subset_size > 1.:\n",
    "                raise ValueError(\"subset_size must be a float whose value is in (0., 1.]\")\n",
    "            return cls.__create_subset(images, subset_size)\n",
    "        \n",
    "        return images\n",
    "        \n",
    "    @classmethod\n",
    "    def __create_subset(cls, images, subset_size):\n",
    "        indices = range(len(images))\n",
    "        _, indices = np.unique([int(idx * subset_size) for idx in indices], return_index=True)\n",
    "        return [images[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Vehicle Registration Plate Dataset Registration</font>\n",
    "\n",
    "Custom datasets must also be registered with Detectron2 by implementing a function that returns the items in your dataset and then \"telling\" Detectron2 about this function (see [Register a Dataset](https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html#register-a-dataset)). In addition, datasets may have associated metadata (see [\"Metadata\" for Datasets](https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html#metadata-for-datasets)). The `Datasets` class registers the following datasets with \"thing_classes\", \"thing_colors\", and \"dataset_size\" metadata.\n",
    "\n",
    "* Training Dataset\n",
    "* Validation Dataset\n",
    "* Training Subset\n",
    "* Validation Subset\n",
    "\n",
    "Note: `Datasets` is not instantiable. Call its `detectron2_registration()` class method to register the Vehicle Registration Plate datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(object):\n",
    "    __registered = False\n",
    "    __thing_classes = None\n",
    "    __thing_colors = None\n",
    "    __train_dataset_name = None\n",
    "    __valid_dataset_name = None\n",
    "    __train_dataset_subset_name = None\n",
    "    __valid_dataset_subset_name = None\n",
    "    __train_dataset_size = None\n",
    "    __valid_dataset_size = None\n",
    "    __train_dataset_subset_size = None\n",
    "    __valid_dataset_subset_size = None\n",
    "    __train_dataset_metadata = None\n",
    "    __valid_dataset_metadata = None\n",
    "    __train_dataset_subset_metadata = None\n",
    "    __valid_dataset_subset_metadata = None\n",
    "\n",
    "    @classproperty\n",
    "    def thing_classes(cls):\n",
    "        return Datasets.__thing_classes\n",
    "\n",
    "    @classproperty\n",
    "    def thing_colors(cls):\n",
    "        return Datasets.__thing_colors\n",
    "\n",
    "    @classproperty\n",
    "    def train_dataset_name(cls):\n",
    "        return Datasets.__train_dataset_name\n",
    "\n",
    "    @classproperty\n",
    "    def valid_dataset_name(cls):\n",
    "        return Datasets.__valid_dataset_name\n",
    "\n",
    "    @classproperty\n",
    "    def train_dataset_subset_name(cls):\n",
    "        return Datasets.__train_dataset_subset_name\n",
    "\n",
    "    @classproperty\n",
    "    def valid_dataset_subset_name(cls):\n",
    "        return Datasets.__valid_dataset_subset_name\n",
    "\n",
    "    @classproperty\n",
    "    def train_dataset_size(cls):\n",
    "        return Datasets.__train_dataset_size\n",
    "\n",
    "    @classproperty\n",
    "    def valid_dataset_size(cls):\n",
    "        return Datasets.__valid_dataset_size\n",
    "\n",
    "    @classproperty\n",
    "    def train_dataset_subset_size(cls):\n",
    "        return Datasets.__train_dataset_subset_size\n",
    "\n",
    "    @classproperty\n",
    "    def valid_dataset_subset_size(cls):\n",
    "        return Datasets.__valid_dataset_subset_size\n",
    "\n",
    "    @classproperty\n",
    "    def train_dataset_metadata(cls):\n",
    "        return Datasets.__train_dataset_metadata\n",
    "\n",
    "    @classproperty\n",
    "    def valid_dataset_metadata(cls):\n",
    "        return Datasets.__valid_dataset_metadata\n",
    "\n",
    "    @classproperty\n",
    "    def train_dataset_subset_metadata(cls):\n",
    "        return Datasets.__train_dataset_subset_metadata\n",
    "\n",
    "    @classproperty\n",
    "    def valid_dataset_subset_metadata(cls):\n",
    "        return Datasets.__valid_dataset_subset_metadata\n",
    "   \n",
    "    def __init__(self):\n",
    "        raise Exception(\"Datasets is not instantiable. Call Datasets.detectron2_registration().\")\n",
    "        \n",
    "    @classmethod\n",
    "    def __register_dataset(cls, name, func, size):\n",
    "        detectron2.data.DatasetCatalog.register(name=name, func=func)\n",
    "        metadata = detectron2.data.MetadataCatalog.get(name)\n",
    "        metadata.thing_classes = Datasets.__thing_classes\n",
    "        metadata.thing_colors = Datasets.__thing_colors\n",
    "        metadata.dataset_size = size\n",
    "        return metadata\n",
    "\n",
    "    @classmethod\n",
    "    def detectron2_registration(cls):\n",
    "        if not Datasets.__registered:\n",
    "            print(\"Registering datasets ...\")\n",
    "            Datasets.__registered = True\n",
    "            Datasets.__thing_classes = [\"lic. plate\"]\n",
    "            Datasets.__thing_colors = [(255, 140, 0)]\n",
    "\n",
    "            train_dataset_func = lambda: Datastore.instance.get_train_image_dict_list()\n",
    "            Datasets.__train_dataset_size = len(train_dataset_func())\n",
    "            Datasets.__train_dataset_name = \"vehicle_registration_plate/train/all\"\n",
    "            Datasets.__train_dataset_metadata = cls.__register_dataset(\n",
    "                name = Datasets.__train_dataset_name,\n",
    "                func = train_dataset_func,\n",
    "                size = Datasets.__train_dataset_size\n",
    "            )\n",
    "\n",
    "            valid_dataset_func = lambda: Datastore.instance.get_valid_image_dict_list()\n",
    "            Datasets.__valid_dataset_size = len(valid_dataset_func())\n",
    "            Datasets.__valid_dataset_name = \"vehicle_registration_plate/valid/all\"\n",
    "            Datasets.__valid_dataset_metadata = cls.__register_dataset(\n",
    "                name = Datasets.__valid_dataset_name,\n",
    "                func = valid_dataset_func,\n",
    "                size = Datasets.__valid_dataset_size\n",
    "            )\n",
    "\n",
    "            train_dataset_subset_func = lambda: Datastore.instance.get_train_image_dict_list(subset_size=0.05)\n",
    "            Datasets.__train_dataset_subset_size = len(train_dataset_subset_func())\n",
    "            Datasets.__train_dataset_subset_name = \"vehicle_registration_plate/train/subset\"\n",
    "            Datasets.__train_dataset_subset_metadata = cls.__register_dataset(\n",
    "                name = Datasets.__train_dataset_subset_name,\n",
    "                func = train_dataset_subset_func,\n",
    "                size = Datasets.__train_dataset_subset_size\n",
    "            )\n",
    "\n",
    "            valid_dataset_subset_func = lambda: Datastore.instance.get_valid_image_dict_list(subset_size=0.05)\n",
    "            Datasets.__valid_dataset_subset_size = len(valid_dataset_subset_func())\n",
    "            Datasets.__valid_dataset_subset_name = \"vehicle_registration_plate/valid/subset\"\n",
    "            Datasets.__valid_dataset_subset_metadata = cls.__register_dataset(\n",
    "                name = Datasets.__valid_dataset_subset_name,\n",
    "                func = valid_dataset_subset_func,\n",
    "                size = Datasets.__valid_dataset_subset_size\n",
    "            )\n",
    "        \n",
    "\n",
    "Datasets.detectron2_registration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Dataset Analysis</font>\n",
    "\n",
    "It is useful to understand the dataset's image dimensions and aspect ratios when creating transforms and data loader collaters. The `analyze_image_dims()` function displays width, height, and aspect ratio histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_dims():\n",
    "    datastore = Datastore.instance\n",
    "    \n",
    "    # create list of training and validation samples\n",
    "    images = []\n",
    "    images.extend(datastore.get_train_image_dict_list())\n",
    "    images.extend(datastore.get_valid_image_dict_list())\n",
    "\n",
    "    def get_image_dims(image):\n",
    "        width = image[\"width\"]\n",
    "        height = image[\"height\"]\n",
    "        return (width, height, float(width)/height)    \n",
    "\n",
    "    image_dims = tuple(zip(*[get_image_dims(image) for image in images]))\n",
    "    \n",
    "    def plot_histogram(ax, xlabel, data):\n",
    "        min = np.min(data)\n",
    "        max = np.max(data)\n",
    "        ave = np.mean(data)\n",
    "        title = f\"{xlabel} Histogram (Min: {min:.2f}, Max: {max:.2f}, Ave: {ave:.2f})\"\n",
    "        ax.hist(data, facecolor=(1., 0.549, 0))\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.set_title(title)\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1)\n",
    "    fig.set_figwidth(6)\n",
    "    fig.set_figheight(9)\n",
    "\n",
    "    xlabels = (\"Width\", \"Height\", \"Aspect Ratio\")\n",
    "    for ax, xlabel, data in zip(axs, xlabels, image_dims):\n",
    "        plot_histogram(ax, xlabel, data)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    del images\n",
    "    del image_dims\n",
    "\n",
    "\n",
    "analyze_image_dims()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font style=\"color:green\">1. Plot Ground Truth Bounding Boxes [20 Points]</font> \n",
    "\n",
    "**You have to show three images from validation data with the bounding boxes.**\n",
    "\n",
    "The plotted images should be similar to the following:\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g1.png'>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g2.png'>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-g3.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Custom Annotator</font>\n",
    "\n",
    "The Detectron2 has a robust [annotation and visualization](https://detectron2.readthedocs.io/en/latest/modules/utils.html#module-detectron2.utils.visualizer)  module. However, to learn how to annotate images in Python I wrote a custom `Annotator` class. This class draws labeled \"ground truth\" and \"predicted\" bounding boxes. Of note is this annotator's ability to scale its annotations according to bounding box sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotator(object):\n",
    "    _min_pad = 3.\n",
    "    _max_pad = 8.\n",
    "    _min_scale = 0.333\n",
    "    _max_scale = 1.\n",
    "    _min_thick = 1.\n",
    "    _max_thick = 5.\n",
    "    _min_box_diag = 40.\n",
    "    _max_box_diag = 200.\n",
    "\n",
    "    @classmethod\n",
    "    def _annotate_image(cls, img_bgr, bboxes, scores=None, color=(255, 140, 0)):\n",
    "        white = (255, 255, 255)\n",
    "        for idx, bbox in enumerate(bboxes):\n",
    "            xminb, yminb, xmaxb, ymaxb = bbox\n",
    "            xminb = int(xminb + 0.5)\n",
    "            yminb = int(yminb + 0.5)\n",
    "            xmaxb = int(xmaxb + 0.5)\n",
    "            ymaxb = int(ymaxb + 0.5)\n",
    "            scale, thick, pad = cls._compute_scaling_factors(xminb, yminb, xmaxb, ymaxb)\n",
    "            cv2.rectangle(img_bgr, (xminb, yminb), (xmaxb, ymaxb), color, thick)\n",
    "\n",
    "            text = \"lic. plate\"\n",
    "            if scores is not None:\n",
    "                text = format(scores[idx], \".0%\")\n",
    "            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_DUPLEX, scale, 1)\n",
    "\n",
    "            xmint = xminb\n",
    "            ymint = yminb - text_size[1] - pad\n",
    "            xmaxt = xminb + text_size[0] + pad\n",
    "            ymaxt = yminb\n",
    "            xtext = xmint + int(pad / 2)\n",
    "            ytext = ymint + text_size[1] + int(pad / 2)\n",
    "            cv2.rectangle(img_bgr, (xmint, ymint), (xmaxt, ymaxt), color, thick)\n",
    "            cv2.rectangle(img_bgr, (xmint, ymint), (xmaxt, ymaxt), color, -1)\n",
    "            cv2.putText(img_bgr, text, (xtext, ytext), cv2.FONT_HERSHEY_DUPLEX, scale, white, 1)\n",
    "        return img_bgr\n",
    "    \n",
    "    @classmethod\n",
    "    def _compute_scaling_factors(cls, xmin, ymin, xmax, ymax):\n",
    "        dx = (xmax - xmin)\n",
    "        dy = (ymax - ymin)\n",
    "        box_diag = math.sqrt(dx * dx + dy * dy)\n",
    "        box_diag = max(min(box_diag, Annotator._max_box_diag), Annotator._min_box_diag)\n",
    "        delta_box_diag = float(Annotator._max_box_diag - Annotator._min_box_diag)\n",
    "        m_pad   = (Annotator._max_pad   - Annotator._min_pad)   / delta_box_diag\n",
    "        m_scale = (Annotator._max_scale - Annotator._min_scale) / delta_box_diag\n",
    "        m_thick = (Annotator._max_thick - Annotator._min_thick) / delta_box_diag\n",
    "        delta_box_diag = float(box_diag - Annotator._min_box_diag)\n",
    "        pad   = Annotator._min_pad   + m_pad   * delta_box_diag\n",
    "        scale = Annotator._min_scale + m_scale * delta_box_diag\n",
    "        thick = Annotator._min_thick + m_thick * delta_box_diag\n",
    "        return scale, int(thick + 0.5), int(pad + 0.5)\n",
    "\n",
    "    @classmethod\n",
    "    def annotate_image_dict(cls, image_dict, color=(0, 140, 255)):\n",
    "        img_bgr = cv2.imread(image_dict[\"file_name\"])\n",
    "        bboxes = [annotation[\"bbox\"] for annotation in image_dict[\"annotations\"]]\n",
    "        return cls._annotate_image(img_bgr, bboxes, None, color)\n",
    "    \n",
    "    @classmethod\n",
    "    def annotate_infer_dict(cls, infer_dict, img_bgr=None, color=((0, 140, 255), (0, 0, 255)), iou_thd=0.5):\n",
    "        if img_bgr is None:\n",
    "            img_bgr = cv2.imread(infer_dict[\"file_name\"])\n",
    "        pred_boxes = infer_dict[\"pred_boxes\"]\n",
    "        pred_scores = infer_dict[\"pred_scores\"]\n",
    "        if \"gt_boxes\" in infer_dict:\n",
    "            missing_boxes = [gt_box for gt_box in infer_dict[\"gt_boxes\"] \n",
    "                             if max([cls._compute_iou(gt_box, pred_box) for pred_box in pred_boxes]) < iou_thd]\n",
    "            cls._annotate_image(img_bgr, missing_boxes, None, color[1])                \n",
    "        return cls._annotate_image(img_bgr, pred_boxes, pred_scores, color[0])\n",
    "\n",
    "    @classmethod\n",
    "    def _compute_idx_max_iou(cls, bbox, bboxes):\n",
    "        iou = [cls._compute_iou(bbox, bbox2) for bbox2 in bboxes]\n",
    "        max_iou = max(iou)\n",
    "        idx_iou = iou.index(max_iou)\n",
    "        return idx_iou, max_iou\n",
    "    \n",
    "    @classmethod\n",
    "    def _compute_iou(cls, bbox1, bbox2):\n",
    "        xmin1, ymin1, xmax1, ymax1 = bbox1\n",
    "        xmin2, ymin2, xmax2, ymax2 = bbox2\n",
    "\n",
    "        xminI = max(xmin1, xmin2)\n",
    "        yminI = max(ymin1, ymin2)\n",
    "        xmaxI = min(xmax1, xmax2)\n",
    "        ymaxI = min(ymax1, ymax2)\n",
    "        \n",
    "        if xmaxI < xminI or ymaxI < yminI:\n",
    "            return 0.\n",
    "        \n",
    "        bbox1_area = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
    "        bbox2_area = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
    "        inter_area = (xmaxI - xminI) * (ymaxI - yminI)\n",
    "        \n",
    "        iou = inter_area / float(bbox1_area + bbox2_area - inter_area)\n",
    "        assert iou >= 0.0\n",
    "        assert iou <= 1.0\n",
    "        return iou\n",
    "    \n",
    "    @classmethod\n",
    "    def show_image(cls, img, is_rgb=False):\n",
    "        plt.figure(figsize=(12,12))\n",
    "        if is_rgb:\n",
    "            plt.imshow(img)\n",
    "        else:\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "image_dict_list = Datastore.instance.get_valid_image_dict_list([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(Annotator.annotate_image_dict(image_dict_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(Annotator.annotate_image_dict(image_dict_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(Annotator.annotate_image_dict(image_dict_list[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font style=\"color:green\">2. Training [25 Points]</font> \n",
    "\n",
    "- **Write your training code in this section.**\n",
    "\n",
    "- **You also have to share ([shared logs example](https://tensorboard.dev/experiment/JRtnsKbwTaq1ow6nPLPGeg)) the loss plot of your training using tensorboard.dev.** \n",
    "\n",
    "How to share TensorBoard logs using tensorboard.dev find [here](https://courses.opencv.org/courses/course-v1:OpenCV+OpenCV-106+2019_T1/courseware/b1c43ffe765246658e537109e188addb/d62572ec8bd344db9aeae81235ede618/4?activate_block_id=block-v1%3AOpenCV%2BOpenCV-106%2B2019_T1%2Btype%40vertical%2Bblock%40398b46ddcd5c465fa52cb4d572ba3229)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Inferencer and Trainer</font>\n",
    "\n",
    "Detectron provides inference and training classes. Nevertheless, to better understand the nuisances of object detection, I implemented my own `Inferencer` and `Trainer` classes. The `Inferencer` class implements a method to perform a COCO evaluation of the model. The `Trainer` class is derived from the `Inferencer` so it has this same capability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inferencer(object):\n",
    "    _output_dir = \"./output\"\n",
    "    _output_name = \"best_model\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        Inferencer._setup_system(cfg)\n",
    "        self._cfg = cfg\n",
    "        self._model = detectron2.modeling.build_model(cfg)\n",
    "        self._output_dir = self._create_output_dir(cfg)\n",
    "        self._coco_evaluator = self._create_coco_evaluator(cfg)\n",
    "        self._test_data_loader = detectron2.data.build_detection_test_loader(cfg, cfg.DATASETS.TEST[0])\n",
    "        self._init_continued(cfg)\n",
    "\n",
    "    def _init_continued(self, cfg):\n",
    "        detectron2.checkpoint.DetectionCheckpointer(self._model).load(\n",
    "            os.path.join(self._output_dir, Inferencer._output_name + \".pth\")\n",
    "        )\n",
    "\n",
    "    def _create_output_dir(self, cfg):\n",
    "        output_dir = os.path.join(Inferencer._output_dir, cfg.OUTPUT_DIR)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        return output_dir\n",
    "    \n",
    "    def _create_coco_evaluator(self, cfg):\n",
    "        output_dir = os.path.join(Inferencer._output_dir, \"CoCo\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        # suppress warning that config constructor is deprecated\n",
    "        with io.capture_output():\n",
    "            return detectron2.evaluation.COCOEvaluator(\n",
    "                dataset_name = cfg.DATASETS.TEST[0],\n",
    "                tasks = cfg,\n",
    "                distributed = False,\n",
    "                output_dir = output_dir\n",
    "            )\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        with io.capture_output() as captured:\n",
    "            results = detectron2.evaluation.inference_on_dataset(\n",
    "                self._model, \n",
    "                self._test_data_loader, \n",
    "                self._coco_evaluator\n",
    "            )\n",
    "        results_dict = results[\"bbox\"]\n",
    "        results_text = [line.strip() for line in captured.stdout.split('\\n') if line.startswith(\"Average\", 1)]\n",
    "        return results_dict, results_text\n",
    "\n",
    "    def inference_image_dict_list(self, image_dict_list, thd=0.4):\n",
    "        mapper = detectron2.data.DatasetMapper(self._cfg, False)\n",
    "        data_loader = detectron2.data.build_detection_test_loader(image_dict_list, mapper=mapper)\n",
    "        return [self.inference_prepared_bgr_image(data_dict[0], image_dict[\"annotations\"], thd) \n",
    "                for data_dict, image_dict in zip(data_loader, image_dict_list)]\n",
    "\n",
    "    def inference_prepared_bgr_image(self, data_dict, annotations=None, thd=0.4):\n",
    "        infer_dict = {}\n",
    "        for key in [\"image_id\", \"file_name\", \"width\", \"height\"]:\n",
    "            if key in data_dict:\n",
    "                infer_dict[key] = data_dict[key]\n",
    "        \n",
    "        if annotations is not None:\n",
    "            infer_dict[\"gt_boxes\"] = [annotation[\"bbox\"] for annotation in annotations]\n",
    "            infer_dict[\"gt_classes\"] = [annotation[\"category_id\"] for annotation in annotations]\n",
    "            \n",
    "        self._model.eval()\n",
    "        inference = self._model([data_dict])\n",
    "        instances = inference[0][\"instances\"]\n",
    "        instances = instances[instances.scores > thd]\n",
    "        device = torch.device(\"cpu\")\n",
    "        \n",
    "        infer_dict[\"pred_boxes\"] = instances.pred_boxes.to(device).tensor.tolist()\n",
    "        infer_dict[\"pred_scores\"] = instances.scores.to(device).tolist()\n",
    "        infer_dict[\"pred_classes\"] = instances.pred_classes.to(device).tolist()\n",
    "\n",
    "        return infer_dict\n",
    "    \n",
    "    def prepare_bgr_image(self, np_ndarray):\n",
    "        width = np_ndarray.shape[1]\n",
    "        height = np_ndarray.shape[0]\n",
    "        aug_input = T.AugInput(np_ndarray)\n",
    "        augmentations = T.AugmentationList(utils.build_augmentation(self._cfg, False))\n",
    "        augmentations(aug_input)\n",
    "        return {\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"image\": torch.as_tensor(np.ascontiguousarray(aug_input.image.transpose(2, 0, 1)))\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def _setup_system(cls, cfg):\n",
    "        seed = cfg.SEED\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.set_printoptions(precision=10)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn_benchmark_enabled = False\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "    \n",
    "class Trainer(Inferencer):\n",
    "    _runs_dir = \"./runs\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "        self._train_data_loader, self._train_data_loader_len = Trainer._get_data_loader(cfg, cfg.DATASETS.TRAIN[0])\n",
    "        self._valid_data_loader, self._valid_data_loader_len = Trainer._get_data_loader(cfg, cfg.DATASETS.TEST[0])\n",
    "\n",
    "    def _init_continued(self, cfg):\n",
    "        self._optimizer = detectron2.solver.build_optimizer(cfg, self._model)\n",
    "        self._scheduler = detectron2.solver.build_lr_scheduler(cfg, self._optimizer)\n",
    "        self._checkpointer = detectron2.checkpoint.DetectionCheckpointer(\n",
    "            self._model, \n",
    "            self._output_dir, \n",
    "            optimizer = self._optimizer, \n",
    "            scheduler = self._scheduler\n",
    "        )\n",
    "        # suppress unwanted warnings that certain model weights are not loaded\n",
    "        # because of differences in the number of classes\n",
    "        with io.capture_output():  \n",
    "            self._checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "    @classmethod\n",
    "    def _get_data_loader(cls, cfg, name):\n",
    "        batch_size = cfg.SOLVER.IMS_PER_BATCH\n",
    "        data_loader = detectron2.data.build_detection_train_loader(\n",
    "            dataset = detectron2.data.DatasetCatalog.get(name),\n",
    "            mapper = detectron2.data.DatasetMapper(cfg, True),\n",
    "            total_batch_size = batch_size\n",
    "        )\n",
    "        dataset_len = detectron2.data.MetadataCatalog.get(name).dataset_size\n",
    "        data_loader_len = int((dataset_len + batch_size - 1) / batch_size)\n",
    "        return data_loader, data_loader_len\n",
    "        \n",
    "    def train(\n",
    "        self, \n",
    "        exp_name, \n",
    "        num_epochs, \n",
    "        no_best_thd=None\n",
    "    ):\n",
    "        import torch.optim as optim\n",
    "        import torch.optim.lr_scheduler as sched\n",
    "        from detectron2.utils.events import EventStorage\n",
    "\n",
    "        def create_tqdm_iter(desc, iterations, unit=\"batch\"):\n",
    "            return tqdm(\n",
    "                range(iterations), \n",
    "                bar_format = \"{l_bar}{bar}| {n_fmt:>4}/{total_fmt:4} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\", \n",
    "                desc = desc, \n",
    "                mininterval = 1., \n",
    "                unit = \"batch\"\n",
    "            )\n",
    "        \n",
    "        def do_train_cycle(writer, train_data_iter, train_loss, epoch_num, iteration):\n",
    "            self._model.train()\n",
    "            cycle_pbar = create_tqdm_iter(f\"Train {epoch_num:02d}\", self._train_data_loader_len)\n",
    "            for _ in cycle_pbar:\n",
    "                iteration += 1\n",
    "                data = next(train_data_iter)\n",
    "                self._optimizer.zero_grad()\n",
    "                loss_dict = self._model(data)\n",
    "                class_loss = loss_dict[\"loss_cls\"].mean()\n",
    "                bregr_loss = loss_dict[\"loss_box_reg\"].mean()\n",
    "                (class_loss + bregr_loss).backward()\n",
    "                ema = train_loss.add(float(class_loss), float(bregr_loss))\n",
    "                loss_text = f\"{{C:{ema[0]:.3f}, R:{ema[1]:.3f}, T:{ema[2]:.3f}}}\"\n",
    "                cycle_pbar.set_postfix(losses=loss_text)\n",
    "                writer.add_scalar(\"param/lr\", self._optimizer.param_groups[0]['lr'], iteration)\n",
    "                writer.add_scalar(\"training/class_loss\", ema[0], iteration)\n",
    "                writer.add_scalar(\"training/regression_loss\", ema[1], iteration)\n",
    "                writer.add_scalar(\"training/total_loss\", ema[2], iteration)\n",
    "                self._optimizer.step()\n",
    "                self._scheduler.step()                    \n",
    "                del loss_dict\n",
    "            cycle_pbar.close()\n",
    "            return iteration\n",
    "        \n",
    "        def do_valid_cycle(writer, valid_data_iter, valid_loss, epoch_num, iteration):\n",
    "            self._model.train()\n",
    "            valid_loss.reset()\n",
    "            cycle_pbar = create_tqdm_iter(f\"Valid {epoch_num:02d}\", self._valid_data_loader_len)\n",
    "            for idx in cycle_pbar:\n",
    "                data = next(valid_data_iter)\n",
    "                with torch.no_grad():        \n",
    "                    loss_dict = self._model(data)\n",
    "                class_loss = loss_dict[\"loss_cls\"].mean()\n",
    "                bregr_loss = loss_dict[\"loss_box_reg\"].mean()\n",
    "                mean = valid_loss.add(float(class_loss), float(bregr_loss))\n",
    "                loss_text = f\"{{C:{mean[0]:.3f}, R:{mean[1]:.3f}, T:{mean[2]:.3f}}}\"\n",
    "                cycle_pbar.set_postfix(losses=loss_text)\n",
    "                del loss_dict\n",
    "            mean = valid_loss.mean\n",
    "            writer.add_scalar(\"validation/class_loss\", mean[0], iteration)\n",
    "            writer.add_scalar(\"validation/regression_loss\", mean[1], iteration)\n",
    "            writer.add_scalar(\"validation/total_loss\", mean[2], iteration)\n",
    "            cycle_pbar.close()\n",
    "                \n",
    "        def do_model_evaluation(writer, iteration):\n",
    "            cycle_pbar = create_tqdm_iter(\"CoCoEval\", 1, \"meval\")\n",
    "            for _ in cycle_pbar:\n",
    "                eval_results, _ = self.evaluate_model()\n",
    "                cycle_pbar.set_postfix(mAP=f\"{eval_results['AP']:.3f}\")\n",
    "            cycle_pbar.close()\n",
    "            for metric, value in eval_results.items():\n",
    "                writer.add_scalar(f\"test/{metric}\", value, iteration)\n",
    "\n",
    "        iteration = -1\n",
    "        min_valid_loss = 1e10\n",
    "        epochs_since_best = 0\n",
    "        prev_best_iteration = -1\n",
    "        train_loss = self.LossEMA()\n",
    "        valid_loss = self.LossMean()\n",
    "        train_data_iter = iter(self._train_data_loader)\n",
    "        valid_data_iter = iter(self._valid_data_loader)\n",
    "        writer = SummaryWriter(log_dir=os.path.join(Trainer._runs_dir, exp_name))\n",
    "        with EventStorage(0):\n",
    "            do_valid_cycle(writer, valid_data_iter, valid_loss, 0, 0)\n",
    "            do_model_evaluation(writer, 0)\n",
    "            for epoch_num in range(1, num_epochs + 1):\n",
    "                iteration = do_train_cycle(writer, train_data_iter, train_loss, epoch_num, iteration)\n",
    "                do_valid_cycle(writer, valid_data_iter, valid_loss, epoch_num, iteration)\n",
    "                do_model_evaluation(writer, iteration)\n",
    "\n",
    "                if min_valid_loss > valid_loss.mean[2]:\n",
    "                    epochs_since_best = 0\n",
    "                    min_valid_loss = valid_loss.mean[2]\n",
    "                    self._checkpointer.save(Inferencer._output_name)\n",
    "                else:\n",
    "                    epochs_since_best += 1\n",
    "                    if no_best_thd is not None and epochs_since_best >= no_best_thd:\n",
    "                        break\n",
    "\n",
    "                best_value = 0 if epochs_since_best > 0 else 1\n",
    "                writer.add_scalar(\"param/best\", best_value, prev_best_iteration + 1)\n",
    "                writer.add_scalar(\"param/best\", best_value, iteration)\n",
    "                prev_best_iteration = iteration\n",
    "\n",
    "        writer.close()\n",
    "        self._model.eval()\n",
    "    \n",
    "    class EMA(object):\n",
    "        def __init__(self, alpha=0.3):\n",
    "            self.reset()\n",
    "            self._alpha = alpha\n",
    "\n",
    "        def reset(self):\n",
    "            self._first = True\n",
    "            self._ema = 0.\n",
    "            \n",
    "        @property\n",
    "        def ema(self):\n",
    "            return self._ema\n",
    "\n",
    "        def add(self, x):\n",
    "            self._ema = x if self._first else (1. - self._alpha) * self._ema + self._alpha * x\n",
    "            self._first = False\n",
    "            return self._ema\n",
    "\n",
    "    class Mean(object):\n",
    "        def __init__(self):\n",
    "            self.reset()\n",
    "\n",
    "        def reset(self):\n",
    "            self._n = 0\n",
    "            self._total = 0.\n",
    "            \n",
    "        @property\n",
    "        def mean(self):\n",
    "            return 0. if self._n == 0 else self._total / self._n\n",
    "\n",
    "        def add(self, x):\n",
    "            self._n += 1\n",
    "            self._total += x\n",
    "            return self.mean\n",
    "\n",
    "    class LossEMA(object):\n",
    "        def __init__(self):\n",
    "            self._class_loss = Trainer.EMA()\n",
    "            self._bregr_loss = Trainer.EMA()\n",
    "\n",
    "        def reset(self):\n",
    "            self._class_loss.reset()\n",
    "            self._bregr_loss.reset()\n",
    "\n",
    "        @property\n",
    "        def ema(self):\n",
    "            class_loss_ema = self._class_loss.ema\n",
    "            bregr_loss_ema = self._bregr_loss.ema\n",
    "            total_loss_ema = class_loss_ema + bregr_loss_ema\n",
    "            return (class_loss_ema, bregr_loss_ema, total_loss_ema)\n",
    "\n",
    "        def add(self, class_loss, bregr_loss):\n",
    "            class_loss_ema = self._class_loss.add(class_loss)\n",
    "            bregr_loss_ema = self._bregr_loss.add(bregr_loss)\n",
    "            total_loss_ema = class_loss_ema + bregr_loss_ema\n",
    "            return (class_loss_ema, bregr_loss_ema, total_loss_ema)\n",
    "        \n",
    "    class LossMean(object):\n",
    "        def __init__(self):\n",
    "            self._class_loss = Trainer.Mean()\n",
    "            self._bregr_loss = Trainer.Mean()\n",
    "\n",
    "        def reset(self):\n",
    "            self._class_loss.reset()\n",
    "            self._bregr_loss.reset()\n",
    "\n",
    "        @property\n",
    "        def mean(self):\n",
    "            class_loss_mean = self._class_loss.mean\n",
    "            bregr_loss_mean = self._bregr_loss.mean\n",
    "            total_loss_mean = class_loss_mean + bregr_loss_mean\n",
    "            return (class_loss_mean, bregr_loss_mean, total_loss_mean)\n",
    "\n",
    "        def add(self, class_loss, bregr_loss):\n",
    "            class_loss_mean = self._class_loss.add(class_loss)\n",
    "            bregr_loss_mean = self._bregr_loss.add(bregr_loss)\n",
    "            total_loss_mean = class_loss_mean + bregr_loss_mean\n",
    "            return (class_loss_mean, bregr_loss_mean, total_loss_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Detectron2 Configurations</font>\n",
    "\n",
    "Detectron2 provides a key-value based configuration system that can be used to obtain standard, common behaviors as explained in the [Configs](https://detectron2.readthedocs.io/en/latest/tutorials/configs.html) documentation. I wrote helper functions to create configurations using either 1) all the images or a subset of them, and 2) no data augmentation or simple data augmentation. By simple data augmentation, I mean data augmentation that is available via configuration key-value pairs. More sophisacated data augmentation can be implemented (see [Data Augmentation](https://detectron2.readthedocs.io/en/latest/tutorials/augmentation.html#))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectron2_cfg(\n",
    "    exp_name,\n",
    "    model_config, \n",
    "    num_epochs,\n",
    "    batch_size,\n",
    "    augment,\n",
    "    subset\n",
    "):\n",
    "    if not subset:\n",
    "        train_dataset_name = Datasets.train_dataset_name\n",
    "        train_dataset_size = Datasets.train_dataset_size\n",
    "        valid_dataset_name = Datasets.valid_dataset_name\n",
    "        valid_dataset_size = Datasets.valid_dataset_size\n",
    "    else:\n",
    "        train_dataset_name = Datasets.train_dataset_subset_name\n",
    "        train_dataset_size = Datasets.train_dataset_subset_size\n",
    "        valid_dataset_name = Datasets.valid_dataset_subset_name\n",
    "        valid_dataset_size = Datasets.valid_dataset_subset_size\n",
    "    \n",
    "    iters_per_epoch = int((train_dataset_size + batch_size - 1) / batch_size)\n",
    "\n",
    "    cfg = detectron2.config.get_cfg()\n",
    "    # suppress unwanted warnings about Detectron2 yaml loading\n",
    "    with io.capture_output():  \n",
    "        cfg.merge_from_file(detectron2.model_zoo.get_config_file(model_config))\n",
    "    cfg.SEED = 42\n",
    "    cfg.DATASETS.TRAIN = (train_dataset_name, )\n",
    "    cfg.DATASETS.TEST = (valid_dataset_name, )\n",
    "    cfg.DATALOADER.NUM_WORKERS = 12\n",
    "    cfg.INPUT.RANDOM_FLIP = \"none\"\n",
    "    cfg.MODEL.WEIGHTS = detectron2.model_zoo.get_checkpoint_url(model_config)\n",
    "    cfg.MODEL.RETINANET.NUM_CLASSES = len(Datasets.thing_classes)\n",
    "    cfg.OUTPUT_DIR = exp_name\n",
    "    cfg.SOLVER.IMS_PER_BATCH = batch_size\n",
    "    cfg.SOLVER.BASE_LR = 0.0001\n",
    "    cfg.SOLVER.MAX_ITER = num_epochs * iters_per_epoch\n",
    "    cfg.SOLVER.WARMUP_ITERS = 2 * iters_per_epoch\n",
    "    cfg.TEST.EVAL_PERIOD = iters_per_epoch\n",
    "    if augment:\n",
    "        cfg.INPUT.CROP.ENABLED = True\n",
    "        cfg.INPUT.CROP.SIZE = [0.9, 0.9]\n",
    "        cfg.INPUT.CROP.TYPE = \"relative_range\"\n",
    "        cfg.INPUT.MIN_SIZE_TRAIN = (640, 800)\n",
    "        cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING = \"range\"\n",
    "        cfg.INPUT.RANDOM_FLIP = \"horizontal\"        \n",
    "    return cfg\n",
    "    \n",
    "def retinanet50_cfg(exp_name, num_epochs, augment, subset):\n",
    "    return detectron2_cfg(\n",
    "        exp_name = exp_name,\n",
    "        model_config = \"COCO-Detection/retinanet_R_50_FPN_3x.yaml\",\n",
    "        num_epochs = num_epochs,\n",
    "        batch_size = 8,\n",
    "        augment = augment,\n",
    "        subset = subset\n",
    "    )\n",
    "    \n",
    "def retinanet101_cfg(exp_name, num_epochs, augment, subset):\n",
    "    return detectron2_cfg(\n",
    "        exp_name = exp_name,\n",
    "        model_config = \"COCO-Detection/retinanet_R_101_FPN_3x.yaml\",\n",
    "        num_epochs = num_epochs,\n",
    "        batch_size = 6,\n",
    "        augment = augment,\n",
    "        subset = subset\n",
    "    )\n",
    "    \n",
    "def fasterrcnnX101_cfg(exp_name, num_epochs, augment, subset):\n",
    "    return detectron2_cfg(\n",
    "        exp_name = exp_name,\n",
    "        model_config = \"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\",\n",
    "        num_epochs = num_epochs,\n",
    "        batch_size = 3,\n",
    "        augment = augment,\n",
    "        subset = subset\n",
    "    )\n",
    "    \n",
    "def create_config(exp_name, cfg_func, num_epochs=0, augment=False, subset=False):\n",
    "    return cfg_func(exp_name, num_epochs, augment, subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Training Experiments and TensorBoard Logs</font>\n",
    "\n",
    "This cell shows the output of a sample training experiment and contains a link to the TensorBoard logs generated by all experiments.\n",
    "\n",
    "### <font style=\"color:orange\">Training Experiments</font>\n",
    "\n",
    "Training experiments are conducted via the `conduct_experiment` function. TQDM progress bars are used to display training progress. The following output is the first ten epochs of training the Retina-50 model without data augmentation on the entire dataset.\n",
    "\n",
    "```\n",
    "Valid 00: 100%|██████████|   49/49   [00:29<00:00,  1.67batch/s, losses={C:1.279, R:0.563, T:1.842}]\n",
    "CoCoEval: 100%|██████████|    1/1    [00:20<00:00, 20.65s/batch, mAP=0.000]\n",
    "Train 01: 100%|██████████|  664/664  [16:47<00:00,  1.52s/batch, losses={C:0.298, R:0.326, T:0.624}]\n",
    "Valid 01: 100%|██████████|   49/49   [00:29<00:00,  1.68batch/s, losses={C:0.251, R:0.206, T:0.457}]\n",
    "CoCoEval: 100%|██████████|    1/1    [00:21<00:00, 21.65s/batch, mAP=50.006]\n",
    "Train 02: 100%|██████████|  664/664  [16:45<00:00,  1.51s/batch, losses={C:0.223, R:0.290, T:0.513}]\n",
    "Valid 02: 100%|██████████|   49/49   [00:28<00:00,  1.70batch/s, losses={C:0.173, R:0.186, T:0.359}]\n",
    "CoCoEval: 100%|██████████|    1/1    [00:21<00:00, 21.43s/batch, mAP=55.580]\n",
    "Train 03: 100%|██████████|  664/664  [16:47<00:00,  1.52s/batch, losses={C:0.161, R:0.246, T:0.407}]\n",
    "Valid 03: 100%|██████████|   49/49   [00:29<00:00,  1.69batch/s, losses={C:0.143, R:0.175, T:0.317}]\n",
    "CoCoEval: 100%|██████████|    1/1    [00:21<00:00, 21.12s/batch, mAP=59.602]\n",
    "Train 04: 100%|██████████|  664/664  [16:44<00:00,  1.51s/batch, losses={C:0.129, R:0.169, T:0.298}]\n",
    "Valid 04: 100%|██████████|   49/49   [00:29<00:00,  1.68batch/s, losses={C:0.131, R:0.177, T:0.308}]\n",
    "CoCoEval: 100%|██████████|    1/1    [00:20<00:00, 21.00s/batch, mAP=60.837]\n",
    "Train 05: 100%|██████████|  664/664  [16:48<00:00,  1.52s/batch, losses={C:0.212, R:0.282, T:0.495}]\n",
    "Valid 05: 100%|██████████|   49/49   [00:29<00:00,  1.66batch/s, losses={C:0.128, R:0.174, T:0.302}]\n",
    "CoCoEval: 100%|██████████|    1/1    [00:21<00:00, 21.10s/batch, mAP=60.461]\n",
    "Train 06: 100%|██████████|  664/664  [16:43<00:00,  1.51s/batch, losses={C:0.170, R:0.296, T:0.466}]\n",
    "Valid 06: 100%|██████████|   49/49   [00:29<00:00,  1.69batch/s, losses={C:0.131, R:0.165, T:0.295}]\n",
    "CoCoEval: 100%|██████████|    1/1    [00:21<00:00, 21.08s/batch, mAP=59.819]\n",
    "Train 07: 100%|██████████|  664/664  [16:43<00:00,  1.51s/batch, losses={C:0.136, R:0.213, T:0.348}]\n",
    "Valid 07: 100%|██████████|   49/49   [00:28<00:00,  1.70batch/s, losses={C:0.123, R:0.172, T:0.295}]\n",
    "CoCoEval: 100%|██████████|    1/1    [00:21<00:00, 21.03s/batch, mAP=60.495]\n",
    "Train 08: 100%|██████████|  664/664  [16:48<00:00,  1.52s/batch, losses={C:0.094, R:0.176, T:0.270}]\n",
    "Valid 08: 100%|██████████|   49/49   [00:29<00:00,  1.69batch/s, losses={C:0.117, R:0.162, T:0.278}]\n",
    "CoCoEval: 100%|██████████|    1/1    [00:20<00:00, 20.98s/batch, mAP=61.254]\n",
    "Train 09: 100%|██████████|  664/664  [16:46<00:00,  1.52s/batch, losses={C:0.101, R:0.206, T:0.307}]\n",
    "Valid 09: 100%|██████████|   49/49   [00:28<00:00,  1.69batch/s, losses={C:0.129, R:0.170, T:0.298}]\n",
    "CoCoEval: 100%|██████████|    1/1    [00:21<00:00, 21.20s/batch, mAP=59.804]\n",
    "Train 10: 100%|██████████|  664/664  [16:44<00:00,  1.51s/batch, losses={C:0.106, R:0.235, T:0.341}]\n",
    "Valid 10: 100%|██████████|   49/49   [00:29<00:00,  1.65batch/s, losses={C:0.117, R:0.163, T:0.280}]\n",
    "CoCoEval: 100%|██████████|    1/1    [00:20<00:00, 20.98s/batch, mAP=60.289]\n",
    "```\n",
    "\n",
    "### <font style=\"color:orange\">TensorBoard Logs</font>\n",
    "\n",
    "The TensorBoard logs may be viewed by clicking this [link](https://tensorboard.dev/experiment/IG8BBp0BSK6g7fOpiaNj3Q/#scalars&_smoothingWeight=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def conduct_experiment(exp_name, cfg_func, num_epochs, no_best_thd, augment, subset):\n",
    "    cfg = create_config(exp_name, cfg_func, num_epochs, augment, subset)\n",
    "    trainer = Trainer(cfg)\n",
    "    trainer.train(exp_name, num_epochs, no_best_thd)\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    return cfg\n",
    "\n",
    "\n",
    "# Note: uncomment the following lines to run an experiment\n",
    "\n",
    "# conduct_experiment(\"AAA-RetinaNet50-SUB\",       retinanet50_cfg, 100, 10, False, True )\n",
    "# conduct_experiment(\"AAB-RetinaNet50-ALL\",       retinanet50_cfg, 100, 10, False, False)\n",
    "# conduct_experiment(\"AAC-RetinaNet50-AUG\",       retinanet50_cfg, 100, 10, True,  False)\n",
    "# conduct_experiment(\"ABA-RetinaNet101-SUB\",     retinanet101_cfg, 100, 10, False, True )\n",
    "# conduct_experiment(\"ABB-RetinaNet101-ALL\",     retinanet101_cfg, 100, 10, False, False)\n",
    "# conduct_experiment(\"ABC-RetinaNet101-AUG\",     retinanet101_cfg, 100, 10, True,  False)\n",
    "# conduct_experiment(\"BAA-FasterRCNNX101-SUB\", fasterrcnnX101_cfg, 100, 10, False, True )\n",
    "# conduct_experiment(\"BAB-FasterRCNNX101-ALL\", fasterrcnnX101_cfg, 100, 10, False, False)\n",
    "# conduct_experiment(\"BAC-FasterRCNNX101-AUG\", fasterrcnnX101_cfg, 100, 10, True,  False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font style=\"color:green\">3. Inference [15 Points]</font> \n",
    "\n",
    "**You have to make predictions from your trained model on three images from the validation dataset.**\n",
    "\n",
    "The plotted images should be similar to the following:\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p1.png'>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p2.png'>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w10-p3.png'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Detecting and Annotating License Plates in Sample Validation Images</font>\n",
    "\n",
    "The following cell creates an \"inferencer\", loads the trained model with the highest mAP, and annotates predictions on the same three samples used in Section 1 to plot ground truth bounding boxes. The subsequent three cells perform a prediction on each sample and annotate the detected license plates.\n",
    "\n",
    "**Note:** If a ground truth box does not have a corresponding prediction w/ IoU >= 0.5, then a red bounding box is drawn with the label \"lic. plate\". This allows one to easily see license plates not detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict_list = Datastore.instance.get_valid_image_dict_list([4, 5, 6])\n",
    "inferencer = Inferencer(create_config(\"ABC-RetinaNet101-AUG\", retinanet101_cfg))\n",
    "infer_dict_list = inferencer.inference_image_dict_list(image_dict_list, 0.5)\n",
    "\n",
    "for infer_dict in infer_dict_list:\n",
    "    for k,v in infer_dict.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(Annotator.annotate_infer_dict(infer_dict_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(Annotator.annotate_infer_dict(infer_dict_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotator.show_image(Annotator.annotate_infer_dict(infer_dict_list[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font style=\"color:green\">4. COCO Detection Evaluation [25 Points]</font> \n",
    "\n",
    "**You have to evaluate your detection model on COCO detection evaluation metric.**\n",
    "\n",
    "For your reference here is the coco evaluation metric chart:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/03/c3-w9-coco_metric.png\">\n",
    "\n",
    "---\n",
    "\n",
    "#### <font style=\"color:red\">The expected `AP` (primary challenge metric) is more than `0.5`.</font>\n",
    "\n",
    "**The expected output should look similar to the following:**\n",
    "\n",
    "```\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.550\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.886\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.629\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.256\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.653\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.627\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.504\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.629\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.633\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.380\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.722\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.704\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Detection Model Evaluation</font>\n",
    "\n",
    "The following cell calls the inferencer's `evaluate_model()` method to evaluate the model with the highest mAP on the COCO detection evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict, results_text = inferencer.evaluate_model()\n",
    "\n",
    "print(\"Results: %s\\n\" % \", \".join([f\"{k}={v:.3f}\" for k,v in results_dict.items()]))\n",
    "print(\"Results - Human Readable Text\")\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print('\\n'.join(results_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:green\">5. Run Inference on a Video [15 Points]</font>\n",
    "\n",
    "#### [Download the Input Video](https://www.dropbox.com/s/g88o4dx18zpgn8k/projet3-input-video.mp4?dl=1)\n",
    "\n",
    "**You have to run inference on a video.** \n",
    "\n",
    "You can download the video from [here](https://www.dropbox.com/s/g88o4dx18zpgn8k/projet3-input-video.mp4?dl=1).\n",
    "\n",
    "#### <font style=\"color:red\">Upload the output video on youtube and share the link. Do not upload the video in the lab.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo, display\n",
    "video = YouTubeVideo(\"18HWHCevFdU\", width=640, height=360)\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your output video should have a bounding box around the vehicle registration plate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = YouTubeVideo(\"5SgCuee7AMs\", width=640, height=360)\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Detection and Annotating a Video</font>\n",
    "\n",
    "I modified the sample function to read the source video frame-by-frame, detect and annotate license plates in each frame, and create an output video. I uploaded this video to YouTube and it may be viewed in this notebook's last cell. Again, Detectron2 provides this capability (see the [detectron2.utils.video_visualizer module](https://detectron2.readthedocs.io/en/latest/modules/utils.html#module-detectron2.utils.video_visualizer) documentation for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_video(inferencer, src_video_path, dst_video_path, frames_dir=None, vis_thd=0.4):\n",
    "    \"\"\"\n",
    "    Create a new video by annotating license plates in the source video.\n",
    "    \n",
    "    src_video_path (str): path of source video in which to annotate\n",
    "    dst_video_path (str): path to destination video in which to create\n",
    "    frames_dir optional(str): if specified, direction to write annotated frames\n",
    "    \"\"\"\n",
    "    \n",
    "    if frames_dir is not None:\n",
    "        os.makedirs(frames_dir, exist_ok=True)\n",
    "    \n",
    "    # create a video reader\n",
    "    reader = cv2.VideoCapture(src_video_path)\n",
    "    if not reader.isOpened(): \n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "        \n",
    "    # get source video attributes\n",
    "    width = int(reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frames_per_second = float(reader.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # create a video writer\n",
    "    writer = cv2.VideoWriter(\n",
    "        filename = dst_video_path,\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "        fps =  frames_per_second,\n",
    "        frameSize = (width, height),\n",
    "        isColor=True,\n",
    "    )\n",
    "    \n",
    "    # for each frame in the source video ...\n",
    "    count = 0\n",
    "    while reader.isOpened():\n",
    "        # read the frame\n",
    "        ret, frame = reader.read()\n",
    "        if ret:\n",
    "            # detect and annotate license plates\n",
    "            frame_dict = inferencer.prepare_bgr_image(frame)\n",
    "            infer_dict = inferencer.inference_prepared_bgr_image(frame_dict, thd=vis_thd)\n",
    "            frame = Annotator.annotate_infer_dict(infer_dict, frame)\n",
    "            \n",
    "            # write frame to destination video\n",
    "            writer.write(frame)\n",
    "            \n",
    "            # if specified, write frame to frames directory\n",
    "            if frames_dir is not None:\n",
    "                name = f\"frame{count:04d}.jpg\"\n",
    "                path = os.path.join(frames_dir, name)\n",
    "                cv2.imwrite(path, frame)\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # free reader and writer resources\n",
    "    reader.release()\n",
    "    writer.release()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Uncomment the first line to download the video to inference and annotate.\n",
    "\n",
    "# ! wget -O project3-input-video.mp4 https://www.dropbox.com/s/g88o4dx18zpgn8k/projet3-input-video.mp4?dl=1\n",
    "frames_dir = os.path.join(Inferencer._output_dir, \"VideoFrames\")\n",
    "annotate_video(inferencer, \"project3-input-video.mp4\", \"project3-output-video-retinanet101-aug.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:orange\">Annotated Video Using RetinaNet101 Model w/ Data Augmentation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video = YouTubeVideo(\"oMURlILCDqo\", width=640, height=360)\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:orange\">Annotated Video Using Faster R-CNN X101 Model w/o Data Augmentation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video = YouTubeVideo(\"741phKYWYVM\", width=640, height=360)\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:orange\">Conclusions</font>\n",
    "\n",
    "This project fine-tuned a pretrained RetinaNet model with the Feature Pyramid Network backbone on top of feedforward ResNet-50 and ResNet-101 architectures. For comparision, it also fine-tuned a faster R-CNN on top of a feedforward ResNext-101 architecture. The following table summarizes their mean average precision (mAP) with and without data augmentation (DA). The impact of Detectron2 configuration-based data augmentation was minimal.\n",
    "\n",
    "|Model|mAP w/o DA|mAP w/ DA|\n",
    "|:---|:---:|:---:|\n",
    "|RetinaNet-50|0.613|0.615|\n",
    "|RetinaNet-101|0.617|0.625|\n",
    "|Faster R-CNN-X101|0.602|†|\n",
    "\n",
    "† The last experiment, BAC-FasterRCNNX101-AUG, crashed after training for approximately 9.5 hours. Because it does not appear that this experiment will produce a model with the highest mAP, training was abandoned rather than implement code to restart the model from its last checkpoint.\n",
    "\n",
    "Regarding frameworks, the upfront time investigating them is well spent before selecting one for a project. Checking whether the framework is actively maintained and rapid prototyping before commitment is prudent. Lastly, avoid frameworks that lack high-quality documentation.\n",
    "\n",
    "I am impressed with FaceBook AI's Detectron2 framework and would use it again on another project. It has the following benefits.\n",
    "\n",
    "* Actively maintained.\n",
    "* High quality documentation.\n",
    "* Well commented, well structured source code.\n",
    "* There are many articles and blog posts on inferencing and training with it.\n",
    "* There are several public Jupyter notebook examples.\n",
    "\n",
    "##  <font style=\"color:orange\">Future Investigation</font>\n",
    "\n",
    "The following areas warrant future exploration.\n",
    "\n",
    "* The use of different, i.e., non-default, PyTorch and Detectron2's optimizers and learning rate schedulers.\n",
    "* Comparison of non-RetinaNet object detection models (see [Detectron2's Model Zoo](https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md#coco-object-detection-baselines)).\n",
    "* The use of additional Detectron2 data augmentation to mitigate overfitting via a custom [Dataset Mapper](https://detectron2.readthedocs.io/en/latest/modules/data.html?highlight=DatasetMapper#detectron2.data.DatasetMapper) or integration of [Albumentations](https://albumentations.ai/) data augmentation transforms, e.g., [denilv/detectron2-helpers](https://github.com/denilv/detectron2-helpers/blob/master/dummy_albu_mapper.py).\n",
    "* The RetinaNet models detected the large, but not the small partially occluded, license plate in the first test image. The fully trained Faster R-CNN model detected the small partially occluded, but not the large, license plate. Furthermore, Faster R-CNN returned more confident predictions than RetinaNet. Hence, futher exploration between these models is warranted.\n",
    "* Annotate other YouTube videos, e.g., [Driving Downtwon - New York City 4K - USA](https://youtu.be/7HaJArMDKgI), [Driving Downtown - Chicago Main Street 4K - USA](https://youtu.be/gEKwMXGFimk).\n",
    "\n",
    "##  <font style=\"color:orange\">External Project Links</font>\n",
    "\n",
    "For convenience, links to this project's GitHub repository, TensorBoard logs, and YouTube videos are copied below.\n",
    "\n",
    "* [GitHub Repository](https://github.com/blazingcayenne/deep_learning_with_pytorch_project3)\n",
    "* [TensorBoard Logs](https://tensorboard.dev/experiment/IG8BBp0BSK6g7fOpiaNj3Q/#scalars&_smoothingWeight=0)\n",
    "* [RetinaNet101 Video](https://youtu.be/oMURlILCDqo), [Faster R-CNN X101 Video](https://youtu.be/741phKYWYVM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
